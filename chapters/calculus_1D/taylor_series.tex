\section{Taylor series}
\subsection{Introduction}
Say we wish to calculate the value of $\cos(x)$ at some non-trivial value, e.g. $x=\frac{\pi}{7}$, or $x=1.0423$. Of course today we can simply use a computer or a calculator - but how do these even calculate such values? Moreover, a lot of these values are non-algebraic, meaning that we can't express them as decimal fractions to an infinite precision. What if for some important calculation we need to know the value of $\eu^{3.153}$ down to 15 digits, but our calculator only shows 10 digits after the period?

We can answer all these questions by using \emph{approximations} instead of precise values. For example, we know values such as $\sin\left(\frac{\pi}{2}\right)$ with infinite precision (it's exactly $1$), but in the case of less trivial values such as the ones discussed above, we can instead settle for an approximation, as long as it is \textit{close enough} to the actual value for our needs. By ``close enough'' we essentially mean \textit{within some given error range}, e.g. in the case of $\eu^{3.153}$ above, 15 digits of precision means that we want that the value we get is no more than $10^{-15}$ away from the actual value. Let us write this mathematically: if we denote $a$ as our approximation of $\eu^{3.153}$, then
\[
  \lvert a-\eu^{3.153}\rvert \leq 10^{-15}.
\]
More generally, given a value $y$ and and we want to know it to within an \emph{error range} $\Delta$, then for an approximation $a$ to be acceptable, the following must be true:
\begin{equation}
  \lvert a-y \rvert \leq \Delta.
  \label{eq:error_range}
\end{equation}

A good method to approximate functions would allow us to get as precise as we wish, given that we put enough ``work'' into finding an approximated value, e.g. we could use it to get an approximation of $\eu^{3.153}$ up to $\Delta=10^{-15}$, but we could also use it to get an approximation up to $\Delta=10^{-20}$ or $\Delta=10^{-30}$ or any other value - we will simply have to do more calculations to reach such low error ranges. Generaly, the more precise we want our approximation to be, the more calculation we would need to carry out.

Unlike most real functions, polynomials are relatively easy to calculate for any real value $x$, since we simply need to perform the following operations: addition, subtraction, muliplication and raising by an integer power, all operations that are easy for both humans and computers to perform. Ideally, we would like to use polynomials to approximate all functions, e.g. given the function $f(x) = \cos(x)$ it would be great if we could find some polynomial $P(x)$ of a finite order $n$ for which $P(x)=\cos(x)$. Unfortunately such a polynomial does not exist, nor does such polynomials exist for $\sin(x), \sqrt{x}, \exp(x), a^{x}\ \left(a>0\right)$ or any other of the so-called elemntary functions and their compositions (except, of course, polynomials). The reason for this unfortunate reality is rather complicated, but in short it lies in the fact that these functions are \textit{non-algebric}\footnote{technically $\sqrt{x}$ is algebraic, and there are indeed many methods to calculate its values - but non of these methods are as simple as calculating a polynomial\dots except the one we discuss in this section.}.

However, as mentioned before, for each of these functions, we know at least some values with inifinite precision. For example, we know that $\cos(0)=1$ and $\cos\left(\frac{1}{2}\pi\right)=0$. From symmetry we thus know that $\cos(\pi)=-1$ and $\cos\left(\frac{3}{4}\pi\right)=0$. Since $\cos(x)$ is periodic, i.e. $\cos\left(x+2\pi k\right)=\cos(x)$ for any $k\in\mathbb{Z}$, we actually know infinitely many values of the function. For $\exp(x)$ we know that $\EU{0}=1$. \autoref{tab:function_values_infinite_precision} lists some known values of common elemntary functions.

\begin{table}[htpb]
	\centering
	\caption{Some known values of common elemntary functions. Each such value is known to an infinite precision.}
	\label{tab:function_values_infinite_precision}
	\begin{NiceTabular}{lll}[
			cell-space-limits=3pt, code-before=\rowcolors{1}{\tabcol!15}{\tabcol!10} \rowcolor{\tabcol!50}{1}
		]
		\toprule
		\RowStyle{\bfseries} Function & $x$ & $f(x)$\\
		\midrule
    $\sqrt{x}$ & $4$ & $2$\\
    $\sin(x)$ & $0$ & $0$\\
    $\cos(x)$ & $0$ & $1$\\
    $\tan(x)$ & $0$ & $0$\\
    $\exp(x)$ & $0$ & $1$\\
    $\log(x)$ & $1$ & $0$\\
		\bottomrule
	\end{NiceTabular}
\end{table}

We can use this knowledge to constract a polynomial with $n$ terms, which approximates the function's value for any $x\in\Rs$ to whatever precision we wish, given that a specific condition is met. We will describe this condition later, but first let us use an example function to construct such a polynomial: $\exp(x)$. Since we only know $\EU{0}$ with infinite precision, we can use it as a simple approximation of $\exp(x)$, which we denote $T_{0}(x)$:
\begin{equation}
  T_{0}(x) = 1.
  \label{eq:zero_order_approx_exp}
\end{equation}
(see \autoref{fig:zero_order_approx_exp})

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      graph2d,
      width=10cm, height=10cm,
      xmin=-2, xmax=4,
      ymin=-1, ymax=17,
      major grid style={black!10},
      minor grid style={black!5},
    ]
      \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
      \addplot[function, xblue] {1} node[above, pos=0.85]{$T_{0}(x)=1$};
      \foreach \xi in {-3.5, -3, ..., -0.5, 0.5, 1, ..., 3} {
        \edef\temp{\noexpand\draw[thick, dashed] (\xi, 1) -- (\xi, {exp(\xi)});}
          \temp
        }
    \end{axis}
  \end{tikzpicture}
  \caption{The 0th-order approximation $\exp(x)=1$. The dashed lines show $\Delta_{0}(x)$ as $x$ diverges from $0$.}
  \label{fig:zero_order_approx_exp}
\end{figure}

We call $T_{0}(x)$ the \emph{0th-order approximation} of $\exp(x)$. Obviously, this is not a particularly good approximation: it's perhaps ok-ish for values \textit{really} close to $x=0$, but rapidly diverges from the actual value of $\exp(x)$ as we change $x$. More precisely, the error $\Delta_{0}(x)$ is
\begin{equation}
  \Delta_{0}(x) = \lvert \exp(x) - T_{0}(x) \rvert = \lvert \exp(x)-1 \rvert,
  \label{eq:exp_err_0}
\end{equation}
i.e. the error essentialy grows as $\exp(x)$ which is\dots not great.

Ok, then perhaps we can add another term to the approximation? After all, our notation pretty much suggests that there are more terms we can use\footnote{more like \textit{screams} it.}. We know that close to a point $a$ a differential function $f(x)$ behaves pretty much like its \textit{derivative} at that point, $f'(a)$. We also know the value of the derivative of $\exp(x)$ at $x=0$, i.e. $\exp'(0)=1$, so we can add a line of slope $m=1$ to our approximation, yielding:
\begin{equation}
  \exp(x) = 1 + x\cdot\exp'\left(0\right) = 1+x.
  \label{eq:first_order_approx_exp}
\end{equation}

Unsurprisingly, we call $T_{1}(x)$ the \emph{1st-order approximation} of $\exp(x)$. Looking at \autoref{fig:first_order_approx_exp} we see that this approximation is better than the previous one, at least for values near $x=0$. In other words, the error $\Delta_{1}$ behaves a bit better than $\Delta_{0}$:
\begin{equation}
  \Delta_{1}(x) = \lvert \exp(x) - T_{1}(x) \rvert = \lvert \exp(x)-x-1 \rvert.
  \label{eq:exp_err_1}
\end{equation}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      graph2d,
      width=10cm, height=10cm,
      xmin=-5, xmax=5,
      ymin=-4, ymax=19,
      major grid style={black!10},
      minor grid style={black!5},
    ]
      \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
      \addplot[function, xblue] {1+x} node[below, pos=0.9, rotate=27]{$T_{1}(x)$};
    \end{axis}
  \end{tikzpicture}
  \caption{The 1st-order approximation $T_{1}(x)=1+x$.}
  \label{fig:first_order_approx_exp}
\end{figure}

Similarly to $\Delta_{0}(x)$, $\Delta_{1}(x)$ also generally grows as $\exp(x)$. However, closer to $x=0$ it behaves like $\exp(x)-x$, so still not great - but definitely an improvement over $\Delta_{0}(x)$. In practical terms, this means that by using $T_{!}(x)$ we would generally get better approximations than $T_{0}(x)$, at least close to $x=0$. But we can do better!

The next step is of course to add an $x^{2}$ term. The second derivative of $\exp(x)$ is also $\exp(x)$, and at $x=0$ also equal to $1$. This time we will divide the term by $2$ (for know just accept it as is, it would be explained when we formalize the method). Altogether, we get
\begin{equation}
  T_{2}(x) = 1 + x + \frac{x^{2}}{2}.
  \label{eq:second_order_approx_exp}
\end{equation}
(see \autoref{fig:second_order_approx_exp})

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      graph2d,
      width=10cm, height=10cm,
      xmin=-4, xmax=4,
      ymin=-1, ymax=13,
      major grid style={black!10},
      minor grid style={black!5},
    ]
      \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
      \addplot[function, xblue] {1+x+x^2/2} node[below, pos=0.7, yshift=-9mm]{$T_{2}(x)$};
    \end{axis}
  \end{tikzpicture}
  \caption{The 2nd-order approximation $T_{2}(x)=1+x+\frac{x^{2}}{2}$.}
  \label{fig:second_order_approx_exp}
\end{figure}

Definitely an improvement, but why stop here? We know all the derivatives of $\exp(x)$ at $x=0$: they are all $1$, since $\exp'(x)=\exp(x)$. We can continue adding power terms, yielding the following general approximation:
\begin{equation}
  T_{n}(x) = 1 + x + \frac{x^{2}}{2} + \frac{x^{3}}{3!} + \frac{x^{4}}{4!} + \cdots + \frac{x^{n}}{n!} = \sum\limits_{k=0}^{n}\frac{x^{k}}{k!}.
  \label{eq:n_order_approx_exp}
\end{equation}
(again, for now you should just accept the coefficients $\frac{1}{n!}$, they would be explained soon)

The higher $n$ is, the more term are in the approximation and it gets more precise (see \autoref{fig:exp_approx}). Of course this means that we need to carry out more calculations, as suggested earlier.

\newcommand{\taylorExp}[1]{
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        graph2d,
        width=7.5cm, height=6cm,
        xmin=-5, xmax=5,
        ymin=-4, ymax=13,
        major grid style={black!10},
        minor grid style={black!5},
        declare function={expTaylor(\k,\x)=\x^\k/factorial(\k);},
        ]
        \addplot[function, xred] {exp(x)} node[above, pos=0.1, xshift=-5mm]{$\exp(x)$};
        \pgfmathtruncatemacro{\n}{#1}
        \addplot[function, xblue, summand=expTaylor] {1+sum(\n,\x)};
        \node[draw=xblue, thick, rounded corners, fill=white, align=left, text=xblue] at (-4,10) {$n=\n$};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}
  \hfill
}

% \begin{figure}
%   \centering
%   \taylorExp{1}
%   \taylorExp{2}
%   \taylorExp{3}
%   \taylorExp{4}
%   \taylorExp{5}
%   \taylorExp{6}
%   \taylorExp{7}
%   \taylorExp{8}
%   \caption{The function $\exp(x)$ and its approximation $T_{n}(x)$ for several values of $n$.}
%   \label{fig:exp_approx}
% \end{figure}

This kind of approximation, where we use a polynomial to approximate a real function, is called a \emph{Taylor series} (sometimes also \emph{Taylor expansion}). To calculate the Taylor series of a real function $f(x)$, we must first select a value of $a\in\Rs$ for which we know $f(a)$ with infinite precision (e.g. $x=0$ in the case of $\exp(x)$). The function $f$ must also be infinitely differentiable at $a$, i.e. the $k$-th order derivative $f^{(k)}(a)$ must exist for any $k\in\mathbb{N}$, and we must know its value with inifinite precision. If these conditions are met, then the Taylor series of order $n$ of $f(x)$ at $x=a$ is given by
\begin{equation}
  T_{n}(x) = \sum\limits_{k=0}^{n}\frac{f^{(k)}(a)}{k!}(x-a)^{k}.
  \label{eq:taylor_series}
\end{equation}
We say that a Taylor series as described above is \textit{expanded about} the point $x=a$.

\begin{note}{Maclaurin series}{}
  A Taylor series with $a=0$, i.e. where the series is expanded about $x=0$, is called a \emph{Maclaurin series}.
\end{note}

\begin{example}{Taylor exansion of $\exp(x)$}{}
  Let us verify that using \autoref{eq:taylor_series} indeed yields \autoref{eq:n_order_approx_exp}: since we're using $a=0$, we need to know the value $\exp^{(k)}(0)$ for any $k\in\mathbb{N}$. This is pretty simple: the $k$-th derivative of $\exp(x)$ is $\exp(x)$, and therefore $\exp^{(k)}(0)=1$ for any $k\in\mathbb{N}$. Substituting $a=0$ and $f^{(k)}(x)=1$ to \autoref{eq:taylor_series} gives back \autoref{eq:n_order_approx_exp}.
\end{example}

\begin{example}{Taylor series of $\sin(x)$ and $\cos(x)$}{}
  Now let us calculate the Taylor series of $\sin(x)$ and $\cos(x)$, denoting them as $T^{\text{s}}_{n}(x)$ and $T^{\text{c}}_{n}(x)$, respectively. For both the functions we can use $a=0$ since we know the values of the functions at these points: $\sin(0)=0,\ \cos(0)=1$. The $k$-th order derivative of $\sin(x)$ depends on the value of $k$ modulo $4$, i.e. the derivatives are a repeating sequence of $4$ element:
  \[
    \tikzmark{a}{}\sin(x) \xrightarrow[]{\od{}{x}} \cos(x) \xrightarrow[]{\od{}{x}} -\sin(x) \xrightarrow[]{\od{}{x}} -\cos(x)\ \tikzmark{b}{}
  \]
  \tikz[overlay, remember picture]{\draw[thick, ->](pic cs:b) to[out=-10, in=190, looseness=1]node[midway, below]{$\od{}{x}$}(pic cs:a)}

  \vspace{1em}
  Since each \textbf{even} order derivative of $\sin(x)$ is $\pm\sin(x)$, and $\pm\sin(0)=0$, we can ignore these terms, since they will vanish from the sum. We therefore need to ``run'' the sum only for \emph{odd} values, which we can write as $m=2k+1$. It's important to notice that the values of the derivatives ``jump'' between positive and negative valiues, i.e. $+\cos(0),-\cos(0),+\cos(0),-\cos(0),\dots = +1,-1,+1,-1,\dots$. We therefore set the derivative term to be $(-1)^{k}$. Altogether we get
  \[
    T^{\text{s}}_{n}(x) = x-\frac{x^{3}}{3!}+\frac{x^{5}}{5!}-\frac{x^{7}}{7!}+\cdots\pm\frac{x^{2n+1}}{(2n+1)!} = \sum\limits_{k=0}^{n}\frac{(-1)^{k}}{(2k+1)!}x^{2k+1}.
  \]

  Similarly, the Taylor series for $\cos(x)$ would have only half of its terms non-zero, but the parity is opposite in comparison to $\sin(x)$: this time, the odd terms disappear, and we're left with only the even terms. Therefore we use $m=2k$, and altogether get
  \[
    T^{\text{c}}_{n}(x) = 1-\frac{x^{2}}{2!}+\frac{x^{4}}{4!}-\frac{x^{6}}{6!}+\cdots\pm\frac{x^{2n}}{(2n)!} = \sum\limits_{k=0}^{n}\frac{(-1)^{k}}{(2k)!}x^{2k}.
  \]
\end{example}

\tbw{When reaching the proof that in the limit $n\to\infty$ the Taylor series equals the function, mention how this can be used to prove Euler's theorem}

\subsection{Remainder}
Recall that one of the requirements we put on an approximation of a function $f(x)$ is that it can get \textit{as close} to $f(x)$ as we want, for any point $x=a$ on some interval $(b,c)$ where $b<c\in\mathbb{R}$. This means that given an approximation $T_{n}(x)$ we can write the function to be approximated as
\begin{equation}
  f(x) = T_{n}(x) + R_{n}(x),
  \label{eq:approximation_remainder}
\end{equation}
i.e. that the function to be approximated can be written as the some of out approximation $T_{n}(x)$, and a \emph{remainder} which we can make as small as we want. ``As small as we want'' means that
\begin{equation}
  \lim\limits_{n\to\infty}R_{n}(x) = 0,
  \label{eq:remainder_to_zero}
\end{equation}
or writing this in another way
\begin{equation}
  \lim\limits_{n\to \infty}T_{n}(x)=f(x).
  \label{eq:approximation_precise}
\end{equation}
The last equation shows exactly what we expect of $T_{n}(x)$: as we increase $n$ the approximation gets closer and closer to the function $f(x)$ which we are approximating.

In essence, Taylor series do exactly that for functions which are infinitely differentiable on some interval: at the limit $n\to\infty$ they give back the function itself (\autoref{eq:approximation_precise}). This is called \emph{Taylor's theorem}.
\begin{note}{No proof}{}
  We won't prove Taylor's theorem here since it requires some more formal calculus. Instead, we would just take as granted that it is indeed true.
\end{note}

The convergence of the Taylor series allows us to prove some theorems about several functions, one of which we will use soon. For now though we concentrate on the use of Taylor series as approximations. Using approximations is always a balance between getting a more precise approximation on one hand, and not doing too much work on the other hand. For example, if we want to use the Taylor series of $\exp(x)$ to calculate $\exp(3.153)$ up to $10^{-15}$, we want to know the minimum value of $n$ such that the remainder $R_{n}(x)$ is smaller then or equal to $10^{-15}$. Any higher $n$ would also give us an approximation within that range, however it will require us to calculate more terms of $T_{n}(x)$. Therefore, we should find a way to estimate $R_{n}(x)$ as a function of $n$, and determine the minimum amount of $n$ needed for an approximation within the desired range.

\subsection{Coefficients}
