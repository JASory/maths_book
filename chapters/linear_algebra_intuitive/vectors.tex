\section{Vectors}
\subsection{Basics}
\emph{Vectors} are the fundamental objects of linear algebra: the entire field revolves around manipulation of vectors. In this chapter we deal with the so-called \emph{real vectors}, which can be be defined in a geometric way:

\begin{definition}{Real vectors}{real vectors}
	A \textit{real vector} is an object with a \emph{magnitude} (also called \emph{norm}) and a \emph{direction}.
\end{definition}

In this chapter we refer to real vectors simply as \textit{vectors}.

\begin{example}{Real vectors}{real vectors}
	The following are all vectors in 2-dimensional space depicted as arrows:
  
	\vspace{1em}
	\centering
	\begin{tikzpicture}
		\draw[vector, xred] (0,0) -- ++(2,3);
		\draw[vector, xblue] (-1,0) -- ++(-1,2);
		\draw[vector, xgreen] (0,-1) -- ++(-3,0);
		\draw[vector, xpurple] (2,0) -- ++(-1,-3);
		\draw[vector, xorange] (-4,2) -- ++(0,-4);
		\draw[vector, black] (-7,1) -- ++(1,-1);
	\end{tikzpicture}
\end{example}

Vectors are usually denoted in one of the following ways:

\begin{descitemize}
	\setlength\itemsep{1em}
	\addtolength{\itemindent}{5mm}
	\item[Arrow above letter] $\vec{u},\ \vec{v},\ \vec{x},\ \vec{a},\ \dots$
	\item[Bold letter] $\bm{u},\ \bm{v},\ \bm{x},\ \bm{a},\ \dots$
	\item[Bar below letter] $\underline{u},\ \underline{v},\ \underline{x},\ \underline{a},\ \dots$
\end{descitemize}

In this book we use the first notation style, i.e. an arrow above the letter. In addition vectors will almost always be denoted using lowercase Lating script.

When discussing vectors in a single context, we always consider them starting at the same point, called the \emph{origin}, and \emph{translating} (moving) vectors around in space does not change their properties: only their norms and directions matter.

\begin{example}{Real vectors}{real vectors}
	The vectors from the previous translated (moved) such that their origins all lie on the same point:
  
	\vspace{1em}
	\centering
	\begin{tikzpicture}
		\draw[vector, xred] (0,0) -- ++(2,3);
		\draw[vector, xblue] (0,0) -- ++(-1,2);
		\draw[vector, xgreen] (0,0) -- ++(-3,0);
		\draw[vector, xpurple] (0,0) -- ++(-1,-3);
		\draw[vector, xorange] (0,0) -- ++(0,-4);
		\draw[vector, black] (0,0) -- ++(1,-1);
		\fill (0,0) circle (0.05);
	\end{tikzpicture}
\end{example}

A vector can be scaled by a real number $\alpha$: when this happens, its norm is multiplied by $\alpha$ while its direction stays the same. We call $\alpha$ a \emph{scalar}.

\begin{example}{Scaling vectors}{scaling vectors}
	The following vector $\vec{v}$ scaled by different scalars $\alpha=2,2.5,-1,-2$:

	\centering
	\begin{tikzpicture}[every node/.style={midway, left, xshift=-2mm}]
		\Large
		\draw[vector, xred] (0,0) -- ++(1.5,1) node {$\vec{v}$};
		\draw[vector, xblue] (2,0) -- ++(3,2) node {$2\cdot \vec{v}$};
		\draw[vector, xpurple] (4.5,0) -- ++(3.75,2.5) node {$2.5\cdot \vec{v}$};
		\draw[stealth-, thick, xgreen!85!black] (7.5,0) -- ++(1.5,1) node {$-1\cdot \vec{v}$};
		\draw[stealth-, thick, black] (9.5,0) -- ++(3,2) node {$-2\cdot \vec{v}$};
	\end{tikzpicture}
\end{example}

\begin{note}{Negative scale}{negative scale}
	As can be seen in the example above, when scaling a vector by a negative amount its direction reverses. However, we consider two opposing direction (i.e. directions that are $\ang{180}$ apart) as being the same direction.
\end{note}

In this book we use the following notation for the norm of a vector $\vec{v}$: $\norm{v}$.

A vector $\vec{v}$ with norm $\norm{v}=1$ is called a \emph{unit vector}, and is usually denoted by replacing the arrow symbol by a hat symbol: $\hat{v}$. Any vector (except $\vec{0}$) can be scaled into a unit vector by scaling  the vector by $1$ over its own norm, i.e.
\begin{equation}
	\hat{v} = \frac{1}{\norm{v}}\vec{v}.
	\label{eq:normalized vector}
\end{equation}
The result of normalization is a vector of unit norm which points in the same direction of the original vector.

Two vectors can be added together to yield a third vector: $\vu+\vv=\vw$. To find $\vw$ we use the following procedure (depicted in \autoref{fig:vector addition geometric}):
% The items need to be typeset without the chapter number
\begin{enumerate}
	\item Move (translate) $\vv$ such that its origin lies on the head of $\vu$.
	\item The vector $\vw$ is the vector drawn from the origin of $\vu$ to the head of $\vv$.
\end{enumerate}

\renewcommand\thesubfigure{\arabic{subfigure}}
\begin{figure}[h]
	\centering
	 \begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\coordinate (O) at (0,0);
			\coordinate (u) at (-2,1);
			\coordinate (v) at (1.5,1);
			\coordinate (w) at ($(u)+(v)$);
			\draw[vector, xred] (O) -- (u) node[above left] {$\vec{u}$};
			\draw[vector, xblue] (O) -- (v) node[above right] {$\vec{v}$};
			\draworigin
		\end{tikzpicture}
		\caption{The vectors $\vu$ and $\vv$.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\draw[vector, xred] (O) -- (u) node[above left] {$\vec{u}$};
			\draw[vector, xblue] (u) -- ++(v) node[above right] {$\vec{v}$};
			\draworigin
		\end{tikzpicture}
		\caption{Translating $\vv$ such that its origin lies at the head of $\vu$.}
	\end{subfigure}

	\vspace{3em}
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\draw[vector, xred] (O) -- (u) node[above left] {$\vec{u}$};
			\draw[vector, xblue] (u) -- ++(v) node[above right] {$\vec{v}$};
			\draw[vector, xpurple] (O) -- (w) node[right, yshift=-2mm] {$\vec{w}$};
			\draworigin
		\end{tikzpicture}
		\caption{Drawing the vector $\vw$ from the origin to the head of $\vv$.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\draw[vector, xred] (O) -- (u) node[above left] {$\vec{u}$};
			\draw[vector, xblue] (O) -- (v) node[above right] {$\vec{v}$};
			\draw[vector, xpurple] (O) -- (w) node[above] {$\vec{w}$};
			\draworigin
		\end{tikzpicture}
		\caption{Showing all three vectors.}
	\end{subfigure}
	\caption{Vector addition.}
	\label{fig:vector addition geometric}
\end{figure}

The addition of vectors as depicted here is commutative, i.e. $\vu+\vv = \vv+\vu$. This can be seen by using the \emph{parallogram law of vector addition} as depicted in \autoref{fig:parallelogram}: drawing the two vectors $\vu, \vv$ and their translated copies (each such that its origin lies on the other vector's head) results in a parallelogram.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\draw[vector, xred] (O) -- (u) node[above left] {$\vec{u}$};
		\draw[vector, xblue] (O) -- (v) node[above right] {$\vec{v}$};
		\draw[vector, xred] (v) -- ++(u);
		\draw[vector, xblue] (u) -- ++(v);
		\draw[vector, xpurple] (O) -- (w) node[above] {$\vec{w}$};
		\draworigin
	\end{tikzpicture}
	\caption{The parallogram law of vector addition.}
	\label{fig:parallelogram}
\end{figure}

An important vector is the \emph{zero-vector}, denoted as $\vec{0}$. The zero-vector has a unique property: it is neutral in respect to vector addition, i.e. for any vector $\vec{v}$,
\begin{equation}
	\vec{v} + \vec{0} = \vec{v}.
	\label{eq:zero-vector}
\end{equation}
(we also say that $\vec{0}$ is the \emph{additive identity} in respect to vectors.)

Any vector $\vec{v}$ always has an \emph{opposite} vector, denoted $-\vec{v}$. The addition of a vector and its opposite always result in the zero-vector, i.e.
\begin{equation}
	\vec{v} + \left( -\vec{v} \right) = \vec{0}.
	\label{eq:opposite vector}
\end{equation}

\subsection{Components}
Vectors can be decomposed to their components, the number of which depends on the dimension of space we're using: 2-dimensional vectors can be decomposed into 2 components, 3-dimensional vectors can be decomposed into 3 components, etc. To decompose a vector, say $\vec{v}$, we first choose a coordinate system: the most commonly used system, and the one we will use for most of this chapter, is the Cartesian coordinate system. We place the vector in the coordinate system such that its origin lies at the origin of the system. We then draw a perpendicular line from its head to each of the axes in the system (see \autoref{fig:vector components}), the point of interception on each axis is the component of the vector in that axis (we label these points $v_{x},v_{y},v_{z}$ in the case of 2- or 3-dimensional spaces, and generally $v_{1},v_{2},v_{3},\dots$). The vector can then be written as a column using these components:
\begin{equation}
	\vec{v} = \colvec{v_{1};v_{2};\vdots;v_{n}}.
	\label{eq:column vector}
\end{equation}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[every node/.style={font=\large}]
		\pgfmathsetmacro{\ux}{2.5}
		\pgfmathsetmacro{\uy}{2}
		\begin{axis}[
			vector plane,
			width=8cm, height=8cm,
			xmin=-1, xmax=3,
			ymin=-1, ymax=3,
			xticklabels={,},
			yticklabels={,},
			extra x ticks={\ux},
			extra x tick labels={$u_{x}$},
			extra x tick style={color=xred},
			extra y ticks={\uy},
			extra y tick labels={$u_{y}$},
			extra y tick style={color=xred},
			]
			\draw[dashed, black!50] (0,\uy) -- (\ux,\uy) -- (\ux,0);
			\draw (\ux,0.1) -- ({\ux+0.1},0.1) -- ({\ux+0.1},0);
			\draw (0.1,\uy) -- (0.1,{\uy+0.1}) -- (0,{\uy+0.1});
			\draw[vector, xred] (0,0) -- (\ux,\uy) node[above] {$\vec{u}=\colvec{u_{x};u_{y}}$};
			\draw[fill] (0,0) circle[radius=2pt];
		\end{axis}
	\end{tikzpicture}
	\caption{Placing a 2-dimensional vector $\vu$ on the 2-dimensional Cartesian coordinate system, showing its $x$- and $y$-components.}
	\label{fig:vector components}
\end{figure}

\begin{note}{Order of components}{}
	The order of the components of a vector is important, and should always be consistent. In the case of $2$- and $3$-dimensional the order is always $v_{x},v_{y},v_{z}$.
\end{note}

\begin{example}{Vector components in two dimensions}{}
	The following five $2$-dimensional vectors are decomposed each into its $x$- and $y$-components:

	\centering
	\begin{tikzpicture}
		\begin{axis}[
			vector plane,
			width=9cm, height=9cm,
			xmin=-3, xmax=3,
			ymin=-3, ymax=3,
			minor tick num=1,
			]
			\veccomp{u}{-2}{1}{xred}
			\veccomp{v}{1.5}{1}{xblue}
			\veccomp{w}{-0.5}{2}{xpurple}
			\veccomp{a}{0.5}{-2}{xgreen}
			\veccomp{b}{-1}{-2}{xorange}
			\draw[fill] (0,0) circle[radius=2pt];
		\end{axis}
	\end{tikzpicture}
\end{example}

\begin{example}{Vector components in three dimensions}{}
	The following $3$-dimensional vector is decomposed into its $x$-, $y$- and $z$-components:
	(THIS NEEDS TO BE IMPROVED AND FINISHED)

	\centering
	\tdplotsetmaincoords{75}{120}
	\begin{tikzpicture}[
			scale=5,
			tdplot_main_coords,
			vector guide/.style={dashed, thick, gray}
		]
		%standard tikz coordinate definition using x, y, z coords
		\coordinate (O) at (0,0,0);

		%tikz-3dplot coordinate definition using x, y, z coords
		\pgfmathsetmacro{\ax}{0.8}
		\pgfmathsetmacro{\ay}{0.8}
		\pgfmathsetmacro{\az}{0.8}

		\coordinate (P) at (\ax,\ay,\az);

		%draw axes
		\draw[vector] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
		\draw[vector] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
		\draw[vector] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};

		%draw a vector from O to P
		\draw[vector, xred] (O) -- (P);

		%draw guide lines to components
		\draw[vector guide]         (O) -- (\ax,\ay,0);
		\draw[vector guide] (\ax,\ay,0) -- (P);
		\draw[vector guide]         (P) -- (0,0,\az);
		\draw[vector guide] (\ax,\ay,0) -- (0,\ay,0);
		\draw[vector guide] (\ax,\ay,0) -- (0,\ay,0);
		\draw[vector guide] (\ax,\ay,0) -- (\ax,0,0);
		\node[tdplot_main_coords, anchor=east] at (\ax,-0.05,0) {$v_{x}$};
		\node[tdplot_main_coords, anchor=west] at (-0.05,\ay,0) {$v_{y}$};
		\node[tdplot_main_coords, anchor=south] at (0.075,0,\az){$v_{z}$};
	\end{tikzpicture}
\end{example}

The column form of a vector is essentially equivalent to an order list of $n$ real numbers, i.e. $(v_{1},v_{2},\dots,v_{n})$. Why then are we using the column form and not the list form (mostly known as \emph{row vectors})? In fact, we could use either form - and even using both interchangeably - and with only minor adjusments the entire chapter would stay the same as it is now. However, there are some advantages of using only a single form, and consider the other form as a different object altogether. This idea will become clear in future chapters, when discussing \emph{covariant vectors}, \emph{contravarient vectors}, and \emph{tensors}. For now, we stick with the column form of vectors to stay consistent with common notation.

However, the row form of vectors highlights the space in which they exist: $n$-dimensional vectors live in a space we call $\Rs{n}$. Recall from \autoref{chapter:intro} that the set $\Rs{n}$ is a Cartesian product made up of $n$ times the set of real numbers, i.e.
\begin{equation}
	\Rs{n} = \underbrace{\mathbb{R} \times \mathbb{R} \times \cdots \times \mathbb{R}}_{n}.
	\label{eq:Rn}
\end{equation}

Each member of this set is a list of $n$ real numbers, and their order inside the list matters - very similar to vectors, be they in row or column form. For this reason, we refer to $\Rs{n}$ as the space of $n$-dimensional real vectors. As mentioned, in this chapter we use $\Rs{2}$ (the 2-dimensional real space) and $\Rs{3}$ (the 3-dimensional real space) for most ideas and examples.

Looking at vectors in $\Rs{2}$, it is rather straight-forward to calculate their norm: since the origin, the head of the vector and the point $v_{x}$ form a right triangle (see \autoref{fig:norm 2D vector}), we can use the Pythagorean theorem to calculate the norm of the vector, which is equal to the hypotenous of said triangle:
\begin{equation}
	\norm{v} = \sqrt{v_{x}^{2} + v_{y}^{2}}.
	\label{eq:2D vector norm}
\end{equation}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[every node/.style={font=\large}]
		\pgfmathsetmacro{\vx}{2.5}
		\pgfmathsetmacro{\vy}{2}
		\pgfmathsetmacro{\an}{atan(\vy/\vx)}
		\begin{axis}[
			vector plane,
			width=9cm, height=9cm,
			xmin=-1, xmax=3,
			ymin=-1, ymax=3,
			xticklabels={,},
			yticklabels={,},
			]
			\fill[xgreen, fill opacity=0.07] (0,0) -- (\vx,\vy) -- (\vx,0);
			\draw[dashed, black!50] (\vx,\vy) -- (\vx,0);
			\draw (\vx,0.1) -- ({\vx-0.1},0.1) -- ({\vx-0.1},0);
			\draw[vector, black] (0,0) -- node[midway, above, rotate=\an] {$\norm{v}=\sqrt{v_{x}^{2}+v_{y}^{2}}$} (\vx,\vy) node[above] {$\vec{v}=\colvec{v_{x};v_{y}}$};
			\draw[fill] (0,0) circle[radius=2pt];
			\draw[xgreen, ultra thick, decorate, decoration={brace, amplitude=3pt, raise=3pt, mirror}]
			(0,0) -- (\vx,0) node[midway, below, yshift=-7pt]{$v_{x}$};
			\draw[xgreen, ultra thick, decorate, decoration={brace, amplitude=3pt, raise=3pt, mirror}]
			(\vx,0) -- (\vx,\vy) node[midway, right, xshift=7pt]{$v_{y}$};
		\end{axis}
	\end{tikzpicture}
	\caption{Calculating the norm of a 2-dimensional column vector.}
	\label{fig:norm 2D vector}
\end{figure}

In $\Rs{3}$ the norm of a vector $\vec{v}$ is similarily
\begin{equation}
	\norm{v} = \sqrt{v_{x}^{2} + v_{y}^{2} + v_{z}^{2}}.
	\label{eq:norm 3D vector}
\end{equation}

\begin{challange}{Norm of a 3D vector}{}
	Show why \autoref{eq:norm 3D vector} is valid, by calculating the length $AB$ in the following figure, depicting a box of sides $\textcolor{xblue}{\bm{a}},\textcolor{xgreen}{\bm{b}}$ and $\textcolor{xpurple}{\bm{c}}$:

	\centering
	\begin{tikzpicture}[every path/.style={very thick}, node distance=1mm]
		\pgfmathsetmacro{\xside}{4};
		\pgfmathsetmacro{\yside}{2};
		\pgfmathsetmacro{\zside}{3};

		\coordinate (1) at (0,0,0);
		\coordinate (2) at (\xside,0,0);
		\coordinate (3) at (0,\yside,0);
		\coordinate (4) at (\xside,\yside,0);
		\coordinate (5) at (0,0,\zside);
		\coordinate (6) at (\xside,0,\zside);
		\coordinate (7) at (0,\yside,\zside);
		\coordinate (8) at (\xside,\yside,\zside);

		\draw (1) -- (2);
		\draw (1) -- (3);
		\draw (1) -- (5);
		\draw[densely dotted, red] (5) -- (4);
		\draw (5) -- (7);
		\draw (6) -- (8);
		\draw (2) -- (4);
		\draw (2) -- (6);
		\draw (3) -- (4);
		\draw (3) -- (7);
		\draw (4) -- (8);
		\draw (5) -- (6);
		\draw (7) -- (8);

		\node[left=of 5] {$A$};
		\node[above=of 4] {$B$};

		\draw[xblue, thick, decorate, decoration={brace, amplitude=3pt, raise=3pt, mirror}]
		(5) -- (6) node[midway, below, yshift=-5pt]{$a$};
		\draw[xgreen, thick, decorate, decoration={brace, amplitude=3pt, raise=3pt, mirror}]
		(6) -- (2) node[midway, right, xshift=2pt, yshift=-8pt]{$b$};
		\draw[xpurple, thick, decorate, decoration={brace, amplitude=3pt, raise=3pt, mirror}]
		(2) -- (4) node[midway, right, xshift=5pt]{$c$};
	\end{tikzpicture}
\end{challange}

Generalizing the vector norms in $\Rs{2}$ and $\Rs{3}$ to $\Rs{n}$ yields the following form:
\begin{equation}
	\norm{v} = \sqrt{v_{1}^{2} + v_{2}^{2} + v_{3}^{2} + \dots + v_{n}^{2}} = \sqrt{\sum\limits_{i=1}^{n}v_{i}^{2}}.
	\label{eq:norm nD vector}
\end{equation}

\begin{note}{Other norms}{}
	The norm shown here is called the $2$-norm. There are other possible norm that can be defined, and are used in different situations, such as the $1$-norm (also the called \emph{taxicab norm}), general $p$-norm where $p\geq1$ is a real number, the zero-norm, the max-norm, and many others. However, for the purpose of this chapter we use only the standard $2$-norm, since it is the most useful for describing basic concepts of linear algebra and its uses.
\end{note}

Scaling a vector $\vec{v}=\colvec{v_{1};v_{2};\vdots;v_{n}}$ by a real number $\alpha$ is done by multiplying each of its components by $\alpha$, i.e.
\begin{equation}
	\alpha\vec{v} = \colvec{\alpha v_{1};\alpha v_{2};\vdots;\alpha v_{n}}.
	\label{eq:scaling vectors}
\end{equation}

We can prove \autoref{eq:scaling vectors} by directly calculating the norm of a scaled vector $\vec{w}=\alpha\vec{v}$:
\begin{proof}{Scaling a column vector}{}
	Let $\vec{v}=\colvec{v_{1};v_{2};\vdots;v_{n}}$ and $\vec{w}=\colvec{\alpha v_{1};\alpha v_{2};\vdots;\alpha v_{n}}$, where $\alpha\in\mathbb{R}$. Then $\vec{w}$ has the following norm:
	\begin{align*}
		\norm{w} &= \sqrt{\sum\limits_{i=1}^{n}(\alpha v_{i})^{2}}\\
		&= \sqrt{(\alpha v_{1})^{2} + (\alpha v_{2})^{2} + \dots + (\alpha v_{1})^{2}}\\
		&= \sqrt{\alpha^{2}v_{1}^{2} + \alpha^{2}v_{2}^{2} + \dots + \alpha^{2}v_{n}^{2}}\\
		&= \sqrt{\alpha^{2}\left( v_{1}^{2} + v_{2}^{2} + \dots + v_{n}^{2} \right)}\\
		&= \alpha\sqrt{v_{1}^{2} + v_{2}^{2} + \dots + v_{n}^{2}}\\
		&= \alpha\norm{v}.
	\end{align*}

	This shows that indeed $\vec{w}=\alpha\vec{v}$.
\end{proof}

Another idea we can prove in column form is vector normalization (\autoref{eq:normalized vector}), by showing that dividing each component of a vector by its norm gives a vector of unit norm:
\begin{proof}{Norm of a vector}{}
	Let $\vec{v}=\colvec{v_{1};v_{2};\vdots;v_{n}}$. Its norm is then $\norm{v}=\sqrt{v_{1}^{2}+v_{2}^{2}+\dots+v_{n}^{2}}$. Scaling $\vec{v}$ by $\frac{1}{\norm{v}}$ yields
	\begin{equation*}
		\hat{v} = \frac{1}{\norm{v}}\colvec{v_{1};v_{2};\vdots;v_{n}} = \frac{1}{\sqrt{v_{1}^{2}+v_{2}^{2}+\dots+v_{n}^{2}}}\colvec{v_{1};v_{2};\vdots;v_{n}}
	\end{equation*}

	The norm of $\hat{v}$ is therefore
	\begin{align*}
		\left\| \hat{v} \right\| &= \sqrt{\frac{v_{1}^{2}}{v_{1}^{2}+v_{2}^{2}+\dots+v_{n}^{2}} + \frac{v_{2}^{2}}{v_{1}^{2}+v_{2}^{2}+\dots+v_{n}^{2}} + \dots + \frac{v_{n}^{2}}{v_{1}^{2}+v_{2}^{2}+\dots+v_{n}^{2}}}\\
		&= \sqrt{\frac{1}{v_{1}^{2}+v_{2}^{2}+\dots+v_{n}^{2}}\left(v_{1}^{2}+v_{2}^{2}+\dots+v_{n}^{2} \right)}\\
		&= \sqrt{1} = 1,
	\end{align*}

	i.e. $\hat{v}$ is indeed a unit vector.
\end{proof}

\begin{example}{Normalizing a vector}{normalizing a vector}
	Let's normalize the vector $\vec{v}=\colvec{0;4;-3}$. Its norm is
	\[
		\norm{v} = \sqrt{0^{2}+4^{2}+(-3)^{2}} = \sqrt{0+16+9} = \sqrt{25} = 5.
	\]
	Therefore $\hat{v}$ (the normalized $\vec{v}$) is
	\[
		\hat{v} = \colvec{0;\frac{4}{5};-\frac{3}{5}}.
	\]

	By calculating the norm of $\hat{v}$ directly, we can see that it is indeed a unit vector:
	\begin{align*}
		\left\|\hat{v}\right\| = \sqrt{0^{2} + \frac{4^{2}}{5^{2}} + \frac{3^{2}}{5^{2}}} = \sqrt{\frac{0^{2}+4^{2}+3^{2}}{5^{2}}} = \sqrt{\frac{16+9}{25}} = \sqrt{\frac{25}{25}} = \sqrt{1} = 1.
	\end{align*}
\end{example}

The addition of two column vectors $\vec{u}=\colvec{u_{1};u_{2};\vdots;u_{n}}$ and $\vec{v}=\colvec{v_{1};v_{2};\vdots;v_{n}}$ is done by adding their respective components together, i.e.
\begin{equation}
	\vec{u} + \vec{v} = \colvec{u_{1}+v_{1};u_{2}+v_{2};\vdots;u_{n}+v_{n}}.
	\label{eq:adding vectors}
\end{equation}

TBW: how this addition is the same as the one shown in \autoref{fig:vector addition geometric}.

\begin{note}{No addition of vectors of different number of components!}{}
	Two vectors can only be added together if they have the same number of components. The addition of vectors with different number of components is undefined.
\end{note}

\subsection{Linear combinations, spans and linear dependency}
As seen above, scaling a vector by a scalar results in a vector that has the same number of dimensions as the original vector. The same is true for adding two vectors: both of them must be of the same dimension, and the result is also a vector of the same dimension. Therefore, any combination of scaling and addition of vectors results in a vector of the same dimension as the original vector(s). This kind of combination is called a \emph{linear combination}.

Let's define linear combinations a little more formaly:

\begin{definition}{Linear combinations}{}
	A linear combination of $n$ vectors $\vec{v}_{1}, \vec{v}_{2}, \dots, \vec{v}_{n}$ of the same dimension, using $n$ scalars $\alpha_{1},\alpha_{2},\dots,\alpha_{n}$, is an expression of the form
	\begin{equation}
		\vec{w} = \alpha_{1}\vec{v}_{1} + \alpha_{2}\vec{v}_{2} + \dots + \alpha_{n}\vec{v}_{n} = \sum\limits_{i=1}^{n}\alpha_{i}\vec{v}_{i}.
		\label{eq:linear combination}
	\end{equation}
\end{definition}

Linear combinations of real vectors have geometric meaningsc: we start with the set of all linear combinations of a single vector $\vec{v}\in\Rs{n}$, i.e.
\begin{equation}
	V = \left\{\alpha\vec{v} \mid \alpha\in\mathbb{R} \right\}.
	\label{eq:span of a single vector}
\end{equation}
The set $V$ represents a line in the direction of $\vec{v}$ going through the origin (see \autoref{fig:span of a single vector}). The set $V$ is itself a vector space of dimension $1$, and as such a \emph{subspace} of $\Rs{n}$. We say that it is the \emph{span} of the vector $\vec{v}$ (i.e. the vector $\vec{v}$ \emph{spans} the subspace $V$).

\def\veccolor{xred}
\tikzset{
	dline/.style={densely dotted, thick, \veccolor!50!gray},
}
\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\begin{tikzpicture}
			\begin{axis}[
					vector plane,
					width=7cm, height=7cm,
					xticklabels={,},
					yticklabels={,},
					declare function={
						ax=2; ay=1;
						bx=0; by=0;
						f(\x)=(by-ay)/(bx-ax)*(\x-ax)+ay;
					},
				]
				\coordinate (A) at ({ax}, {ay});
				\coordinate (B) at ({bx}, {by});
				\draw[vector, \veccolor] (0,0) -- (A) node [above] {$\vec{v}$};
				\draw[dline] (A) -- (6,{f(6)});
				\draw[dline] (B) -- (-6,{f(-6)});
			\end{axis}
		\end{tikzpicture}
		\caption{$\Rs{2}$}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\begin{tikzpicture}
			\begin{axis}[
					width=8cm, height=8cm,
					axis lines=center,
					z buffer=sort,
					xmin=-4, xmax=4,
					ymin=-4, ymax=4,
					zmin=-4, zmax=4,
					xtick=\empty,
					ytick=\empty,
					ztick=\empty,
					view={330}{20},
				]
				% Below surface
				\draw[stealth-, thick] (0,0,-4) -- (0,0,0);
				\draw[dline] (-4,-2.67,-4) -- (0,0,0);

				% Surface
				\addplot3[surf, faceted color=xblue!50!black!50, fill=xblue!20, opacity=0.5, domain=-4:4, samples=7] {0};

				% Above surface
				\draw[axisline] (-4,0,0) -- (4,0,0) node[pos=1.05] {$x$};
				\draw[axisline] (0,-4,0) -- (0,4,0) node[pos=1.075] {$y$};
				\draw[-stealth, thick] (0,0,0) -- (0,0,4) node[pos=1.075] {$z$};
				\draw[dline] (3,2,3) -- (4,2.67,4);
				\draw[vector, \veccolor] (0,0,0) -- (3,2,3) node[above, right] {$\vec{v}$};
				\draw[dashed, black!50] (3,2,3) -- (3,2,0) -- (0,0,0);
			\end{axis}
		\end{tikzpicture}
		\caption{$\Rs{3}$}
	\end{subfigure}
	\caption{The span of a single vector $\color{xred}{\bm{\vec{v}}}$, shown as a dashed line: in $\Rs{2}$ (left) and $\Rs{3}$ (right).}
	\label{fig:span of a single vector}
\end{figure}

Similarily, the set of all linear combinations of two vectors $\vec{u},\vec{v}\in\Rs{n}$ that are not scales of each other (i.e. there is no such $\alpha\in\mathbb{R}$ for which $\vec{v}=\alpha\vec{u}$),
\begin{equation}
	V = \left\{\alpha\vec{u}+\beta\vec{v} \mid \alpha,\beta\in\mathbb{R} \right\},
	\label{eq:span of a two vectors}
\end{equation}
is a plane that goes through the origin (see \autoref{fig:span of two vectors}). Such vectors are also said to be \emph{non-collinear}.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				width=8cm, height=8cm,
				axis lines=center,
				z buffer=sort,
				xmin=-4, xmax=4,
				ymin=-4, ymax=4,
				zmin=-4, zmax=4,
				xtick=\empty,
				ytick=\empty,
				ztick=\empty,
				view={50}{20},
			]
			% Behind surface at z=0
			\draw[stealth-, thick] (0,0,-4) -- (0,0,0);
			\addplot3[surf, faceted color=xgreen!50!black!50, fill=xgreen!20, opacity=0.9, domain=-4:4, y domain=-4:0, samples=7] {0.4*y};

			% Surface at z=0
			\addplot3[surf, faceted color=xblue!50!black!50, fill=xblue!20, opacity=0.9, domain=-4:4, samples=7] {0};

			% Back axis line
			\draw[axisline] (0,0,0) -- (0,4,0) node[pos=1.075] {$y$};
			
			% Infront of surface at z=0
			\addplot3[surf, faceted color=xgreen!50!black!50, fill=xgreen!20, opacity=0.9, domain=-4:4, y domain=0:4, samples=7] {0.4*y};

			% Vectors
			\draw[vector] (0,0,0) -- (-2,3,1.2) node[pos=1.15, fill=white, rounded corners] {$\vec{a}$};
			\draw[vector] (0,0,0) -- (+3,3,1.2) node[pos=1.12, fill=white, rounded corners] {$\vec{b}$};

			% Front axis lines
			\draw[axisline] (-4,0,0) -- (4,0,0) node[pos=1.05] {$x$};
			\draw[stealth-, thick] (0,-4,0) -- (0,0,0);
			\draw[-stealth, thick] (0,0,0) -- (0,0,4) node[pos=1.075] {$z$};
		\end{axis}
	\end{tikzpicture}
	\caption{Two vectors $\vec{a}$ and $\vec{b}$ span a plane (colored green) in $\Rs{3}$. The $xy$-plane (i.e. $z=0$) is shown in blue for emphasis.}
	\label{fig:span of two vectors}
\end{figure}

\begin{example}{Spanning $\bm{\Rs{2}}$ using two non-collinear vectors}{}
	Since any two non-collinear vectors span a 2-dimensional subspace of $\Rs{n}$, in $\Rs{2}$ this means that any vector $\vec{w}$ can be written as a linear combination of any two vectors $\vec{u},\vec{v}$ that are not a scale of each other. For example, we can take the vector
	\[
		\vec{w} = \colvec{7;-1},
	\]
	and write it as a linear combination of any two non-collinear vectors, say
	\[
		\vec{u}=\colvec{2;-3},\ \vec{v}=\colvec{0;5}.
	\]

	The equation which forces the relation is
	\[
		\colvec{7;-1} = \alpha\colvec{2;-3} + \beta\colvec{0;5},
	\]
	and we should solve it for $\alpha$ and $\beta$. This is possible since the equation above is actually a system of two equations in two variables (namely $\alpha$ and $\beta$):
	\[
		\begin{cases}
			\ 7  = 2\alpha,\\
			\ -1 = -3\alpha + 5\beta.
		\end{cases}
	\]
	The solution for the system is $\alpha=3.5$ and $\beta=1.9$, and therefore
	\[
		\colvec{7;-1} = 3.5\colvec{2;-3} + 1.9\colvec{0;5}.
	\]
	
	As the reader, you should verify for yourself the above equation.
\end{example}

Generalizing the example above, any vector $\vec{w}=\colvec{w_{x};w_{y}}$ can be written as a linear combination of two vectors $\vec{u}=\colvec{u_{x};u_{y}}$ and $\vec{v}=\colvec{v_{x};v_{y}}$, as long as $\vec{u}$ and $\vec{v}$ are non-collinear. Let's prove this:
\begin{proof}{$\bm{\Rs{2}}$ is spanned by any two non-collinear vectors in $\bm{\Rs{2}}$}{fd}
	Let $\vec{u},\vec{v}\in\Rs{2}$ be two non-collinear vectors. Their non-collinearity means that the equation
	\begin{equation}
		\vec{u} = \alpha\vec{v}
		\label{eq:collinear vectors}
	\end{equation}
	has no solution, i.e. the system
	\begin{equation}
		\begin{cases}
			&u_{x} = \alpha v_{x}\\
			&u_{y} = \alpha v_{y}
		\end{cases}
		\label{eq:collinear system}
	\end{equation}
	has no solution. The system has solution only when $u_{x}v_{y} = u_{y}v_{x}$, and so the restriction is translated to the simple equation
	\begin{equation}
		u_{x}v_{y} \neq u_{y}v_{x}.
		\label{eq:restriction}
	\end{equation}

	The system which defines $\vec{w}$ as a linear combination of $\vec{u}$ and $\vec{v}$ is
	\begin{equation}
		\begin{cases}
			&w_{x} = \alpha u_{x} + \beta v_{x}\\
			&w_{y} = \alpha u_{y} + \beta v_{y}\\
		\end{cases}
		\label{eq:linear combination of two vectors}
	\end{equation}

	Isolating $\alpha$ using the first equation yields
	\begin{equation}
		\alpha = \frac{w_{x}-\beta v_{x}}{u_{x}},
		\label{eq:isolation1}
	\end{equation}
	and subtituting it into the second equation yields
	\begin{equation}
		\beta = \frac{w_{y}-\alpha u_{y}}{v_{y}} = \frac{w_{y}-\frac{w_{x}-\beta v_{x}}{u_{x}}}{v_{y}},
		\label{eq:isolation2}
	\end{equation}
	which rearranges into
	\begin{equation}
		\beta = \frac{u_{x} w_{y} - u_{y} w_{x}}{u_{x} v_{y} - u_{y} v_{x}},
		\label{eq:test}
	\end{equation}
	and thus
	\begin{equation}
		\alpha = \frac{- v_{x} w_{y} + v_{y} w_{x}}{u_{x} v_{y} - u_{y} v_{x}}.
		\label{eq:test2}
	\end{equation}

	We can see that $\alpha$ and $\beta$ exist iff $u_{x}v_{y}\neq u_{y}v_{x}$, which is guaranteed by \autoref{eq:restriction}. Therefore, $\alpha$ and $\beta$ always exist when $\vec{u}$ and $\vec{v}$ are non-collinear, and thus any vector in $\Rs{2}$ can be written as a linear combination of any two non-collinear vectors in $\Rs{2}$, i.e. any two non-collinear vectors in $\Rs{2}$ span $\Rs{2}$.
\end{proof}

Going a step further, any three vectors $\vec{u},\vec{v},\vec{w}\in\Rs{n}$ that are not coplanar span a 3-dimensional subspace of $\Rs{n}$ going through the origin. To generalize the notion of collinear and coplanar vectors to higher dimensions we introduct the concept of \emph{linear dependency} of a set of vectors:

\begin{definition}{Linear dependent set of vectors}{linear dependency}
	A set of $n$ vectors
	\begin{equation}
		S = \left\{ \vec{v}_{1}, \vec{v}_{2}, \dots, \vec{v}_{n} \right\}
		\label{eq:set of n vectors}
	\end{equation}
	is said to be linearly dependent if there exist a linear combination
	\begin{equation}
		\alpha_{1}\vec{v}_{1} + \alpha_{2}\vec{v}_{2} + \dots + \alpha_{n}\vec{v}_{n} = \vec{0},
		\label{eq:set of n vectors}
	\end{equation}
	and \textbf{at least} one the coefficients $\alpha_{i}\neq0$.
\end{definition}

The following examples shows that the definition above reduces to colinarity and coplanary in the case of $2$ and $3$ vectors:
\begin{example}{Linear dependency of $2$ vectors}{}
	Let $\vec{u}$ and $\vec{v}$ be two linearly dependent vectors in $\Rs{n}$. Then there exist a linear combination
	\[
		\alpha\vec{u} + \beta\vec{v} = \vec{0},
	\]
	with either $\alpha\neq0$ or $\beta\neq0$ (or both). We can look at the different possible cases:
	\begin{itemize}
		\item $\alpha\neq0,\ \beta=0$: in this case $\alpha\vec{u}=\vec{0}$, i.e. $\vec{u}=0$.
		\item $\alpha=0,\ \beta\neq0$: in this case $\beta\vec{v}=\vec{0}$, i.e. $\vec{v}=0$.
		\item $\alpha\neq0,\ \beta\neq0$: in this case we can rearrange the equation and get
			\[
				\vec{u} = -\frac{\beta}{\alpha}\vec{v},
			\]
			i.e. $\vec{u}$ and $\vec{v}$ are scales of each other and thus are collinear.
	\end{itemize}
	What we learn from this is that two vectors form a linearly dependent set if at least one of the is the zero vector, or if they are collinear.

\end{example}

\begin{example}{Linear dependency of $3$ vectors}{}
	Now, let $\vec{u},\vec{v}$ and $\vec{w}$ be three linearly dependent vectors in $\Rs{n}$. Then there exists a linear combination
	\[
		\alpha\vec{u} + \beta\vec{v} + \gamma\vec{w} = \vec{0},
	\]
	with either $\alpha\neq0$ or $\beta\neq0$ or $\gamma\neq0$ or any combination where two of the coefficients are non-zero, or all of the coefficients are non-zero. Again, we look at all the possible cases:
	\begin{itemize}
		\item $\alpha\neq0,\ \beta=\gamma=0$: we get $\alpha\vec{u} = \vec{0}$, thus $\vec{u}=\vec{0}$.
		\item $\alpha=0,\ \beta\neq0,\ \gamma=0$: we get $\beta\vec{v} = \vec{0}$, thus $\vec{v}=\vec{0}$.
		\item $\alpha=\beta=0,\ \gamma\neq0$: we get $\gamma\vec{w} = \vec{0}$, thus $\vec{w}=\vec{0}$.
		\item $\alpha\neq0,\ \beta\neq0, \gamma=0$: we get that $\vec{u}$ and $\vec{v}$ are collinear, since this is exactly as the case for two linearly dependent vectors.
		\item $\alpha\neq0,\ \beta=0, \gamma\neq0$: similar to the previous case, this time $\vec{u}$ and $\vec{w}$ are collinear.
		\item $\alpha=0,\ \beta\neq0, \gamma\neq0$: similar to the previous case, this time $\vec{v}$ and $\vec{w}$ are collinear.
		\item $\alpha\neq0,\ \beta\neq0,\ \gamma\neq0$: by rearranging we get
			\[
				\vec{w} = -\frac{1}{\gamma}\left( \alpha\vec{u} + \beta\vec{v} \right),
			\]
			i.e. $\vec{w}$ lies on the the plane spanned by $\vec{u}$ and $\vec{v}$. If we isolate $\vec{u}$ or $\vec{v}$ instead, we get the same result: the isolated vector is a lienar combination of the other two vectors, and thus lies on the plan spanned by these vectors.
	\end{itemize}
	From this example we learn that three vectors form a linearly dependent set if one or more of the vectors is the zero vector, or if any two vectors in the set are collinear, or if all three vectors are coplanar.
\end{example}

Just like the case of $2$ and $3$ vectors seen above, any set of $m\leq n$ vectors in $\Rs{n}$ that are \textbf{not} linearly dependent span an $m$-dimensional subspace of $\Rs{n}$ (which goes throught the origin) - i.e. any vector $\vec{v}\in\Rs{n}$ can be written as a linear combination of these vectors. We call such a set a \emph{basis set} of $\Rs{n}$.

\begin{example}{Basis sets in $n$ dimensions}{}
	The following three vectors are non coplanar (i.e. they are linearly independent), and thus form a basis set of $\Rs{3}$:
	\[
		B = \left\{ \colvec{0;4;5},\ \colvec{4;2;-2},\ \colvec{1;0;-5} \right\}.
	\]
	This means that any vector in $\Rs{3}$ can be written as a linear combination of these vectors. We can show this by writing a generic vector $\vec{v}=\colvec{x;y;z}\in\Rs{3}$ as a linear combination of the vectors:
	\[
		\vec{v} = \colvec{x;y;z} = \alpha\colvec{0;4;5} + \beta\colvec{4;2;-2} + \gamma\colvec{1;0;-5},
	\]
	which can be expanded to the system of equations
	\[
		\begin{cases}
			& x = \cancel{0\alpha}+4\beta+1\gamma,\\
			& y = 4\alpha+2\beta+\cancel{0\gamma},\\
			& z = 5\alpha-2\beta-5\gamma.
		\end{cases}
	\]

	The solution of the above system gives the coefficients of the linear combination to yield any vector in $\Rs{3}$:
	\begin{align*}
		\alpha &= -\frac{5x}{31} + \frac{9y}{31} - \frac{z}{31},\\
		\beta  &= \frac{10x}{31} - \frac{5y}{62} + \frac{2z}{31},\\
		\gamma &= -\frac{9x}{31} + \frac{10y}{31} - \frac{8z}{31}.
	\end{align*}

	For example, to yield the vector $\vec{v}=\colvec{1;-1;0}$ we sustitute $x=1,\ y=-1,\ z=0$ into the above solutions, and get that the following coefficients are needed:
	\[
		\alpha=-\frac{28}{62},\ \beta=\frac{25}{62},\ \gamma=-\frac{38}{62},
	\]
	i.e.
	\[
		-\frac{28}{62}\colvec{0;4;5} + \frac{25}{62}\colvec{4;2;-2} -\frac{38}{62}\colvec{1;0;-5} = \colvec{1;-1;0}.
	\]
	(you, the reader, should verify this!)
\end{example}

Having described basis sets in somewhat general terms, we can now define them a bit more precisely:

\begin{definition}{Basis sets}{basis sets}
	Let $B$ be a \textbf{linearly independent set} of vectors in $\Rs{n}$. If any vector $\vec{v}\in\mathbb{\Rs{n}}$ can be written as a linear combination of the vectors in $B$, then $B$ is called a basis set of $\Rs{n}$. The \emph{dimension} of $B$ is the number of vectors in $B$.
\end{definition}

The dimension of a basis set $B$ of $\Rs{n}$ is always $n$. In fact, in a later chapter we will see that the dimension of a vector space is defined by the dimension of its basis sets, i.e. given a vector space $V$ and a basis set $B\subseteq V$, the dimension of $V$ is equal to $|B|$, or mathematically
\begin{equation}
	\dim(V) = |B|.
	\label{eq:dimension of a vector space}
\end{equation}

It can be easily shown that any set of vectors in $\Rs{n}$ which has more than $n$ vectors must be a linearly dependent set:

\begin{proof}{Sets with more than $\bm{n}$ vectors in $\bm{\Rs{n}}$}{label}
	Let $S$ be a set of $m\in\mathbb{N}$ vectors in $\Rs{n}$, where $m>n$. Given a vector $\vec{v}\in S$ and the set of all vectors in $S$ except $\vec{v}$ (call this set $\tilde{S}$), there are two possibilities:
	\begin{itemize}
		\item $\tilde{S}$ is a linearly dependent set in $\Rs{n}$. In this case, the addition of $\vec{v}$ doesn't change this fact, i.e. the set $S$ as a whole is linearly dependent.
		\item The set $\tilde{S}$ is linearly independent, and since it has $n$ vectors it forms a basis set of $\Rs{n}$. Therefore, $\vec{v}$ can be written as a linear combination of the vectors in $\tilde{S}$, and thus the inclusion of $\vec{v}$ in $S$ makes $S$ a linearly dependent set.
	\end{itemize}
\end{proof}

\Blindtext

\subsection{Products}
When given two vectors $\vu,\vv\in\Rs{n}$ it is often useful to know the angle between them: if the two vectors are linearly dependent then the angle is either $\ath=0$ if they point in the same direction, or $\ath=\pi$ if the point in opposite directions (remember: we measure angles in radians). Otherwise, the angle $\ath$ can take any value in $(0,\pi)$. Angles are always measured on a plane, and in the case of two linearly independent vectors that plane is of course the one spanned by the two vectors (\autoref{fig:angle between two vectors}).

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				width=8cm, height=8cm,
				axis lines=center,
				z buffer=sort,
				xmin=-4, xmax=4,
				ymin=-4, ymax=4,
				zmin=-4, zmax=4,
				xtick=\empty,
				ytick=\empty,
				ztick=\empty,
				view={50}{10},
			]
			% Behind surface at z=0
			\draw[stealth-, thick] (0,0,-4) -- (0,0,0);
			\addplot3[surf, faceted color=black!25, fill=black!10, opacity=0.95, domain=-4:4, y domain=-4:0, samples=7] {0.5*y};

			% Back axis line
			\draw[axisline] (0,0,0) -- (0,4,0) node[pos=1.075] {$y$};
			
			% Infront of surface at z=0
			\addplot3[surf, faceted color=black!25, fill=black!10, opacity=0.95, domain=-4:4, y domain=0:4, samples=7] {0.5*y};

			% Angle and vectors
			%\draw[thick, fill=white] (0,0,0) -- (1,0.667,0.334) arc (1:0:22.494) -- cycle;
			\draw[vector, xred]  (0,0,0) -- (-1,3,1.5) node [pos=1.1] {$\vec{u}$};
			\draw[vector, xblue] (0,0,0) -- (3,3,1.5) node [pos=1.1] {$\vec{v}$};

			% Front axis lines
			\draw[axisline] (-4,0,0) -- (4,0,0) node[pos=1.05] {$x$};
			\draw[stealth-, thick] (0,-4,0) -- (0,0,0);
			\draw[-stealth, thick] (0,0,0) -- (0,0,4) node[pos=1.075] {$z$};
		\end{axis}
	\end{tikzpicture}
	\caption{The angle between two linearly independent vectors lies on the plane spanned by the vectors.}
	\label{fig:angle between two vectors}
\end{figure}

If considering only the plane the vectors span, we can rotate it such that one of the vectors, say $\vu$, lies horizotally (see \autoref{fig:angle between two vectors in plane}). We then drop a perpendicular line from the head of the $\vu$ to the horizontal vector $\vv$. We call the length from the origin to the intersection point of $\vv$ and the perpendicular line the \emph{projection} of $\vu$ onto $\vv$, and denote it as $\projection$.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\Large
		\coordinate (o) at (0,0);
		\coordinate (u) at (2.5,1.94);
		\coordinate (v) at (3.6,0);
		\coordinate (uv) at ($(u)+(v)$);
		\filldraw[xpurple!20, draw=xpurple, thick] let
		\p1=(u),\p2=(v),\n1={atan2(\y1,\x1)},\n2={atan2(\y2,\x2)}
		in (o) -- ($(o)!1cm!(v)$) arc[start angle=\n2, end angle=\n1, radius=1cm]
		node [text=xpurple, yshift=1pt] at ($(o)!7mm!(uv)$) {$\theta$};

		\draw[vector, xred] (o) -- ++(u) node [pos=1.1] {$\vec{u}$};
		\draw[vector, xblue] (o) -- ++(v) node [pos=1.1] {$\vec{v}$};
		\filldraw (o) circle (0.03);

		\draw[thick, densely dashed] (u) -- ++(0,-1.94);
		\filldraw[black] ($(u)+(0,-1.94)$) circle (0.04);
		\draw [black, thick, decorate, decoration={brace, amplitude=3pt, raise=3pt, mirror}]
		(o) -- ($(u)+(0,-1.94)$) node[midway, below , yshift=-5pt]{$\projection$};
	\end{tikzpicture}
	\caption{The projection of a vector $\vu$ onto another vector $\vv$ in the plane spanned by the two vectors.}
	\label{fig:angle between two vectors in plane}
\end{figure}

Since the origin, the head of $\vu$ and the intersection point of the perpendicular line with $\vv$ form a right triangle, using basic trigonometry we find that the cosine of the angle $\ath$ is
\begin{equation}
	\cos\left(\ath\right) = \frac{\projection}{\gnorm{\vu}}.
	\label{eq:cos from projection}
\end{equation}

We can now use this construct to define a product between $\vu$ and $\vv$: their \emph{scalar product}. We define it as following:
\begin{equation}
	\vu \cdot \vv = \projection \cdot \gnorm{\vv}.
	\label{eq:scalar product}
\end{equation}

Subtituting \autoref{eq:cos from projection} into \autoref{eq:scalar product} gives a very nice relation between the scalar product of two vectors and the angle between them:
\begin{equation}
	\cos\left( \ath \right) = \frac{\vu\cdot\vv}{\gnorm{\vu}\gnorm{\vv}}.
	\label{eq:cos defined via scalar product}
\end{equation}
The angle between the two vectors is then isolated by applying the $\arccos$ function on the right-hand side of \autoref{eq:cos defined via scalar product}. A common form of this equation is the following:
\begin{equation}
	\vu\cdot\vv = \gnorm{\vu}\gnorm{\vv}\cos\left( \ath \right).
	\label{eq:scalar product via cos}
\end{equation}

Note that the scalar product returns a number, i.e. in the terms of linear algebra - a scalar, and hence its name. Since it is commonly denoted with a dot between the two vectors, it is sometimes refered to as the \emph{dot product}. A common notation for the scalar product is the so-called \emph{bracket notation}:
\[
	\langle \vec{a},\vec{b} \rangle.
\]
Sometimes the comma in the notation is replaced by a vertical separator line:
\[
	\langle \vec{a}\mid\vec{b} \rangle.
\]
This notation is very common in physics, and especially quantum physics where it is very useful and helps in simplifying many calculations. This will be discussed in more details in chapter/section TBD.

Later in the section we will examine some common properties of the scalar product, and see how we can calculate it directly from the vectors in their column form. Beofre we do that, let's use what we learned about the scalar product so far to solve some easy problems in the examples below.

\begin{example}{Angle between two vectors}{}
	Find the scalar product of the vectors
	\[
		\vec{a} = \colvec{1;1},\ \vec{b}=\colvec{-1;1}.
	\]

	\textbf{Solution}:

	We first find the angles of both $\vec{a}$ and $\vec{b}$ in relation to the $x$-axis.

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
					vector plane,
					width=7cm, height=5cm,
					xmin=-1.5, xmax=1.5,
					ymin=-0.5, ymax=1.5,
					xtick={-1,...,1},
					ytick={1},
				]
				\draw[thick, fill=xred!20] (0,0) -- (0.6,0) arc (0:45:0.6) -- cycle;
				\draw[thick, fill=xblue!20] (0,0) -- (-0.6,0) arc (180:135:0.6) -- cycle;
				\draw[vector] (0,0) -- (1,1) node[pos=1.1] {$\vec{a}$};
				\draw[vector] (0,0) -- (-1,1) node[pos=1.1] {$\vec{b}$};
				\node at (0.4,0.14) {$\theta_{a}$};
				\node at (-0.4,0.14) {$\theta_{b}$};
			\end{axis}
			\end{tikzpicture}
	\end{figure}

	For both $\vec{a}$ and $\vec{b}$, this angle is $\ang{45}$ since both vectors have the same absolute value for their $x$- and $y$-axes. From the figure it is clear that the angle between the vectors is thus $\ang{90}$ (since $90=180-45-45$), i.e. $\frac{\pi}{2}$ radians.

	The norm of both vectors is also equal: $\norm{a}=\norm{b}=\sqrt{1^{2}+1^{2}}=\sqrt{2}$. Subtituting all this into \autoref{eq:scalar product via cos} yields
	\begin{align*}
		\vec{a}\cdot\vec{b} &= \norm{a}\norm{b}\cos\left(\theta\right)\\
		&= \sqrt{2}\sqrt{2}\cos\left( \frac{\pi}{2} \right)\\
		&= 2\cdot0 = 0.
	\end{align*}
\end{example}

\begin{example}{Scalr product of two vectors}{}
	Another example.
\end{example}

The scalar product of any two vectors $\vec{u},\vec{v}$ has two important properties:
\begin{itemize}
	\item It is commutative, i.e. $\vec{u}\cdot\vec{v} = \vec{v}\cdot\vec{u}$.
	\item It equals zero in only one of two cases:
		\begin{enumerate}
			\item One of the vectors (or both) is the zero vector, or
			\item The angle $\theta$ between the vectors is $\frac{\pi}{2}$, since then $\cos(\theta)=\cos\left(\frac{\pi}{2}\right)=0$.
		\end{enumerate}
\end{itemize}

When the angle between two vectors is $\frac{\pi}{2}$ (remember: this is equivalent to $\ang{90}$), we say that the two vectors are \emph{orhtogonal} to eacth other. Note that in the special case of 2- and 3-dimensional we say that the vectors are \emph{perpendicular} to each other.

TBW: the importance of orthogonality/dot product equaling zero.

\Blindtext
