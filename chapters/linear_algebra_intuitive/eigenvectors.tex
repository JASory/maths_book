\section{Eigenvectors and eigenvalues}
\subsection{Definition}
Some linear transformations have special directions which only scale by the application of the transformation and are not mapped to different directions. Take for example the transformation $T:\Rs{2}\to\Rs{2}$, which scales space by $2$ in the $y$-direction. All vectors pointing in the $y$-direction get scaled by $T$ (namely by a factor of $2$) and still point in the $y$-direction after the application of $T$. All vectors pointing in the $x$-direction do not change at all (i.e. they are "scaled" by a factor of $1$), and of course still point in the $x$-direction after the application of $T$. Any other vector - i.e. those that have both components different than zero - change their direction after the application of $T$ (see \autoref{fig:y_scale_2}).

\begin{figure}
	\centering
	\begin{subfigure}[c]{0.45\textwidth}
		\begin{center}
			\begin{tikzpicture}
				\begin{axis}[
					vector plane,
					width=7.5cm, height=7.5cm,
					xlabel={},
					xticklabels={,},
					yticklabels={,},
					]
					\pgfmathsetmacro{\R}{2}
					\coordinate (O) at (0,0);
					\foreach \k in {0,...,11}{
						\pgfmathsetmacro{\th}{30*\k}
						\edef\temp{\noexpand
							\draw[vector, color=xrainbow\k] (O) -- ({\R*cos(\th)},{\R*sin(\th)});
						;}
						\temp
					}
					\fill (O) circle (0.1);
				\end{axis}
			\end{tikzpicture}
		\end{center}
		\caption{Some vectors.}
		\label{fig:}
	\end{subfigure}
	\begin{subfigure}[c]{0.45\textwidth}
		\begin{center}
			\begin{tikzpicture}
				\begin{axis}[
					vector plane,
					width=7.5cm, height=7.5cm,
					xticklabels={,},
					yticklabels={,},
					]
					\pgfmathsetmacro{\R}{2}
					\coordinate (O) at (0,0);
					\foreach \k in {0,...,11}{
						\pgfmathsetmacro{\th}{30*\k}
						\edef\temp{\noexpand
							\draw[vector, color=xrainbow\k] (O) -- ({\R*cos(\th)},{2*\R*sin(\th)});
						;}
						\temp
					}
					\fill (O) circle (0.1);
				\end{axis}
			\end{tikzpicture}
		\end{center}
		\caption{Same vectors after the application of $T$.}
		\label{fig:}
	\end{subfigure}
	\begin{subfigure}[c]{\textwidth}
		\begin{center}
			\begin{tikzpicture}
				\begin{axis}[
					vector plane,
					width=7.5cm, height=7.5cm,
					xticklabels={,},
					yticklabels={,},
					]
					\pgfmathsetmacro{\R}{2}
					\coordinate (O) at (0,0);
					\foreach \k in {0,...,11}{
						\pgfmathsetmacro{\th}{30*\k}
						\edef\temp{
							\noexpand\draw[vector, color=xrainbow\k] (O) -- ({\R*cos(\th)},{\R*sin(\th)});
							\noexpand\draw[vector, color=xrainbow\k] (O) -- ({\R*cos(\th)},{2*\R*sin(\th)});
						;}
						\temp
					}
					\fill (O) circle (0.1);
				\end{axis}
			\end{tikzpicture}
		\end{center}
		\caption{The vectors before and after the application of $T$ layered on top of eachother.}
		\label{fig:}
	\end{subfigure}
	\caption{Some vectors before and after application of the $y$-scaling transformation $T$. Note how only the vectors pointing in the direction of the $x$- and $y$-axes stay in the same direction, while all the other vectors change their directions.}
	\label{fig:y_scale_2}
\end{figure}

We call such vectors the \emph{eigenvectors} of the transformation. The amount by which the are scaled is then their respective \emph{eigenvalues}.

\begin{note}{Pronounciation}{}
	The word \textit{eigen} is a German word meaning "own" (as in "own rules"), or "self" (as in self-made). We will see how this meaning fits the concept later in the section. The \textit{ei} part it pronounced the same as the English word "eye", and the \textit{g} is pronounced like the \textit{g} in the English word \text{dog} (i.e. unlike the \textit{g} in \textit{generation}).
\end{note}

\begin{example}{Eigenvectors and eigenvalues}{}
	Text here
\end{example}

In matrix form, a vector $\vec{v}$ is an eigenvector of a transformation represented by the matrix $A$, if
\begin{equation}
	A\vec{v} = \lambda\vec{v},
	\label{eq:eigenvector_matrix_form}
\end{equation}
where $\lambda\in\mathbb{R},\ \lambda\neq0$. This kind of equation is typically called an \emph{eigenvector equation}. When there are several eigenvectors for a transformation, each with its distinct eigenvalue, we simply add indeces to all relevant parts:
\begin{equation}
	A\vec{v}_{i} = \lambda_{i}\vec{v}_{i},
	\label{eq:eigenvector_matrix_form_indeces}
\end{equation}
where again $\lambda_{i}\in\mathbb{R}$ and $\lambda_{i}\neq0$.

\begin{note}{The zero-vector}{}
	Although technically the zero vector is indeed "scaled" by any linear transformation - by infinitely many scalars - it is \textbf{not} considered an eigenvector, exactly because of the fact it has no unique eigenvalue.
\end{note}

Before continuing to explore some more examples of eigenvectors, there are two properties\footnote{actually one property and one non-property} of eigenvectors that are important to mention. Given a linear transformation $T$,
\begin{itemize}
	\item A scale of any eigenvector $\vec{v}$ of $T$ is also an eigenvector of $T$, with the same eigenvalue.
		\begin{proof}{Eigenvector scale}{}
			Let $T:\Rs{n}\to\Rs{n}$ be a linear transformation represented by the square matrix $A$, with eigenvector $\vec{v}$ and its respective eigenvalue $\lambda$. Then
			\[
				A\vec{v} = \lambda\vec{v}.
			\]
			Replacing $\vec{v}$ with a scale of itself, i.e. $\vec{u}=\alpha\vec{v}$, then applying $A$ to $\vec{u}$ gives us
			\[
				A\vec{u} \overset{(1)}{=} A \left( \alpha\vec{v} \right) \overset{(2)}{=} \alpha A \vec{v} \overset{(3)}{=} \alpha\lambda\vec{v} \overset{(4)}{=} \lambda\alpha\vec{v} \overset{(5)}{=} \lambda\vec{u}.
			\]
			where
			\begin{enumerate}[label=(\arabic*)]
				\item Substitution of $\vec{u}$ by its definition $\vec{u}=\alpha\vec{v}$.
				\item Due to the linearity of $A$ we can bring $\alpha$ out of the product.
				\item Resulting due to $\vec{v}$ being an eigenvector of $A$.
				\item The product of real numbers is commutative.
				\item Substituting back $\alpha\vec{v}=\vec{u}$.
			\end{enumerate}

			Therefore, $\vec{u}$ is also an eigenvector of $A$ (and thus $T$) with the same eigenvalue $\lambda$ as $\vec{v}$.
		\end{proof}

		Since a linear transformation never has just a single eigenvector but infinitely many (i.e. its entire span), we will refer from now on the \emph{families} of eigenvectors, all pointing in the same direction, represented by a single vector (usually a unit vector, but not necesseraly).

	\item The linear combination of two eigenvectors of $T$ is \textbf{not neccessarily an eigenvector of} $\bm{T}$! For example, consider the above transformation which scales all vectors by $2$ in the $y$-direction: as we saw, any vector in the $x$-direction is an eigenvector of the transformation, and so does any vector in the $y$-direction. Specifically, the vectors $\vec{a}=\colvec{1;0}$ and $\vec{b}=\colvec{0;1}$ are two separate eigenvectors of the transformation (with eigenvalues $1$ and $2$, respectively), however the vector
		\[
			\vec{c} = \vec{a}+\vec{b} = \colvec{1;1}
		\]
		is \textbf{NOT} an eigenvector of the transformation, since
		\[
			\colvec{1;1} \overset{T}{\mapsto} \colvec{1;2} \neq 2\colvec{1;1}.
		\]
\end{itemize}

\subsection{Some examples}
\begin{example}{Eigenvectors and eigenvalues of common linear transformations}{}
	Let's find the eigenvectors and their respective eigenvalues for the some basic linear transformations in $2$-dimensions.

	\begin{descitemize}
		\item[Skew (shear) in the $\bm{x}$-direction] any vector pointing in the $x$-direction is unchanged (i.e. scaled by a factor of $1$), while any other vector changes its direction. Thus, the transformation has only a single family of eigenvectors, which we will represent by the vector $\hat{x}$, and their respective eigenvalue $\lambda=1$.

		\item[Rotation around the origin] no vector (except the zero-vector) is scaled by rotation, and therefore rotations have no eigenvectors. However, in $3$-dimensions any rotation transformation does have an eigenvector, with eigenvalue $\lambda=1$: it is of course the family of vectors pointing in the direction of the axis of rotation. We will discuss this further later in the section.
		
		\item[Reflection across the $\bm{x}$-axis] in this case, any vector pointing in the $x$-axis is not being changed - an eigenvector with eigenvalue $\lambda=1$, and any vector pointing in the $y$-direction is reflected verically, i.e.
			\[
				\colvec{0;\beta} \overset{T}{\mapsto} \colvec{0;-\beta} = -\colvec{0;\beta},
			\]
			and is therefore an eigenvector with eigenvalue $\lambda=-1$. The transformation has no other eigenvectors.

		\item[Reflection across a line going through the origin] much like the previous case, any vector lying on the reflection line will not change ($\lambda=1$), and any vector pointing in an orthogonal direction to the line will flip ($\lambda=-1$). No other eigenvectors exist for this transformation.
	\end{descitemize}
\end{example}

\subsection{Calulating eigenvectors}
Calculating the eigenvectors of a given transformation, and their respective eigenvalues, is a rather easy procedure to perform once one has the transformation in matrix form. However, in order to understand \textit{why} the procedure works it is useful to derive it first, which is what we'll do now.

We take the eigenvector equation (\autoref{eq:eigenvector_matrix_form}) and rearrange it slightly:
\begin{equation}
	A\vec{v} - \lambda\vec{v} = \vec{0}.
	\label{eq:eigenvec_rearranged_1}
\end{equation}

We can then group together all parts which include $\vec{v}$, but we must be careful: $A$ is a matrix while $\lambda$ is a scalar. This means that the term $A-\lambda$ has no meaning, since we haven't defined how to add or subtract matrices and real numbers. We therefore change \autoref{eq:eigenvec_rearranged_1} a bit without changing its validity, by replacing $\lambda\vec{v}$ with $\lambda I\vec{v}$: i.e. instead of scaling $\vec{v}$ by a scalar, we scale it using the matrix $\lambda I$, which yields the same result:

\begin{equation}
	A\vec{v} - \lambda I\vec{v} = \vec{0}.
	\label{eq:eigenvec_rearranged_2}
\end{equation}

\begin{note}{That one weird trick}{}
	If you are not convinced the above trick works, consider the following:
	\[
		\lambda=3, \vec{v}=\colvec{1;5;-7}.
	\]
	Then the direct scaling of $\vec{v}$ by $\lambda$ is
	\[
		\lambda\vec{v} = \colvec{3;15;-21},
	\]
	and scaling it using $\lambda I$ yields
	\[
		\lambda I \vec{v} = \begin{bmatrix}3&0&0\\0&3&0\\0&0&3\end{bmatrix}\colvec{1;5;-7} = \colvec{3;15;-7},
	\]
	i.e. we get exactly the same result.
\end{note}

Grouping together the parts with $\vec{v}$ gives:
\begin{equation}
	\left( A-\lambda I \right)\vec{v} = \vec{0}.
	\label{eq:eigenvec_rearranged_3}
\end{equation}

Note that $A-\lambda I$ is a matrix, with the following form:
\begin{align}
	A-\lambda I &=
		\begin{bNiceMatrix}
			\Ma{1}{1} & \Ma{1}{2} & \cdots & \Ma{1}{n}\\
			\Ma{2}{1} & \Ma{2}{2} & \cdots & \Ma{2}{n}\\
			\vdots & \vdots & \Ddots & \vdots\\
			\Ma{n}{1} & \Ma{n}{2} & \cdots & \Ma{n}{n}
		\end{bNiceMatrix}
		-\lambda
		\begin{bNiceMatrix}
			1 & 0 & \cdots & 0\\
			0 & 1 & \cdots & 0\\
			\vdots & \vdots & \Ddots & \vdots\\
			0 & 0 & \cdots & 1
		\end{bNiceMatrix}\nonumber\\
		&=
		\begin{bNiceMatrix}
			\Ma{1}{1} & \Ma{1}{2} & \cdots & \Ma{1}{n}\\
			\Ma{2}{1} & \Ma{2}{2} & \cdots & \Ma{2}{n}\\
			\vdots & \vdots & \Ddots & \vdots\\
			\Ma{n}{1} & \Ma{n}{2} & \cdots & \Ma{n}{n}
		\end{bNiceMatrix}
		-
		\begin{bNiceMatrix}
			\lambda & 0 & \cdots & 0\\
			0 & \lambda & \cdots & 0\\
			\vdots & \vdots & \Ddots & \vdots\\
			0 & 0 & \cdots & \lambda
		\end{bNiceMatrix}\nonumber\\
		&=
		\begin{bNiceMatrix}
			\Ma{1}{1}-\lambda & \Ma{1}{2} & \cdots & \Ma{1}{n}\\
			\Ma{2}{1} & \Ma{2}{2}-\lambda & \cdots & \Ma{2}{n}\\
			\vdots & \vdots & \Ddots & \vdots\\
			\Ma{n}{1} & \Ma{n}{2} & \cdots & \Ma{n}{n}-\lambda
		\end{bNiceMatrix}.
	\label{eq:A-LI}
\end{align}

\autoref{eq:eigenvec_rearranged_3} tells us that $A-\lambda I$ sends the vector $\vec{v}$ to $\vec{0}$, and therefore $\vec{v}$ is in the kernel of $\left( A-\lambda I \right)$. Sinve by the definition of an eigenvector $\vec{v}\neq\vec{0}$, this means that the kernel of $A-\lambda I$ has more then just the zero vector, and thus
\begin{equation}
	|A-\lambda I| = 0.
	\label{eq:|A-LI|=0}
\end{equation}
(this is derived from \autoref{eq:nullity_rank} and \autoref{eq:rank_det})

Therefore, if we solve \autoref{eq:|A-LI|=0} for $\lambda$, we will get all the values of $\lambda$ for which the eigenvector equation holds, and in turn we get all the eigenvalues $\lambda_{i}$ of the linear transformation represented by $A$. We can then substitute each $\lambda_{i}$ into the eigenvector equation and find its respective eigenvector family.

\begin{example}{Eigenvectors and eigenvalues of a $\bm{2\times2}$ matrix}{}
	The matrix representing the $y$-scaling transformation discussed in the beginning of this section is
	\[
		A = \begin{bmatrix}1&0 \\ 0&2\end{bmatrix}.
	\]
	Therefore, in order to find the eigenvectors of $A$ we solve the equation
	\[
		0 = |A-\lambda I| = \begin{vmatrix}1-\lambda & 0 \\ 0 & 2-\lambda\end{vmatrix} = (1-\lambda)(2-\lambda).
	\]
	In this case there are two solutions: $\lambda_{1}=1$ and $\lambda_{2}=2$. To find their corresponding eigenvectors, we subtitute them into the eigenvectors equation. First $\lambda_{1}$:
	\[
		\begin{bmatrix}1&0 \\ 0&2\end{bmatrix}\colvec{x;y} = \colvec{x;y},
	\]
	which corresponds to the following system of equations:
	\[
		\begin{cases}
			1\cdot x + 0\cdot y = x,\\
			0\cdot x + 2\cdot y = y,
		\end{cases}
	\]
	for which the solution is $x\in\mathbb{R}$ and $y=0$ - i.e. any vector pointing in the $x$-direction. Now for $\lambda_{2}=2$:
	\[
		\begin{bmatrix}1&0 \\ 0&2\end{bmatrix}\colvec{x;y} = 2\colvec{x;y} = \colvec{2x;2y},
	\]
	i.e. the following system of equations:
	\[
		\begin{cases}
			1\cdot x + 0\cdot y = 2x,\\
			0\cdot x + 2\cdot y = 2y.
		\end{cases}
	\]
	The solution is of course $x=0$ and $y\in\mathbb{R}$, meaning any vector pointing in the $y$-direction.
\end{example}

\begin{example}{Eigenvectors and eigenvalues of a $\bm{3\times3}$ matrix}{}
	Let us now calculate the eigenvectors and their respective eigenvalues for the following $3\times3$ matrix:
	\[
		A=
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}.
	\]

	We start by calculating the determinant $|A-\lambda I$:
	\begin{align*}
		|A-\lambda I| &=
		\begin{vmatrix}
			5-\lambda & -6 & -3\\
			0 & -1-\lambda & 0\\
			0 & 2 & 2-\lambda\\
		\end{vmatrix}\\
		&=
		\left(5-\lambda\right)\begin{vmatrix}-1-\lambda & 0 \\ 2 & 2-\lambda\end{vmatrix} -(-6)\cancel{\begin{vmatrix}0&0 \\ 0&2-\lambda\end{vmatrix}} +3 (-3)\cancel{\begin{vmatrix}0&-1-\lambda \\ 0&2\end{vmatrix}}\\
		&= \left(5-\lambda\right) \left( -1-\lambda \right) \left( 2-\lambda \right)\\
	\end{align*}
	The solutions of $|A-\lambda I|=0$ are therefore
	\[
		\lambda_{1}=5,\ \lambda_{2}=-1,\ \lambda_{3}=2.
	\]

	\begin{descitemize}
	\item[$\bm{\lambda_{1}=5}$] solving $A\vec{v} = 5\vec{v}$ yields
	\[
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}\colvec{x;y;z} = 5\colvec{x;y;z} = \colvec{5x;5y;5z},
	\]
	i.e.
	\[
		\begin{cases}
			5x-6y-3z &= 5x,\\
			0x-1y+0z &= 5y,\\
			0x+2y+2z &=5z,
		\end{cases}
	\]
	for which the solution is
	\[
		x\in\mathbb{R},\ y=0,\ z=0.
	\]
	The first family of eigenvectors are the vectors pointing in the $x$-direction, and their respective eigenvalue is $\lambda=5$. We can verify this:
	\[
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}\colvec{x;0;0} = \colvec{5x-6\cdot0-3\cdot0;0x-1\cdot0+0\cdot0;0x+2\cdot0+2\cdot0} = \colvec{5x;0;0} = 5\colvec{x;0;0}.
	\]

	\vspace{1em}
	\item[$\bm{\lambda_{2}=-1}$] solving $A\vec{v}=-\vec{v}$ yields
	\[
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}\colvec{x;y;z} = -\colvec{x;y;z} = \colvec{-x;-y;-z},
	\]
	i.e.
	\[
		\begin{cases}
			5x-6y-3z &= -x,\\
			0x-1y+0z &= -y,\\
			0x+2y+2z &= -z,
		\end{cases}
	\]
	for which the solution is
	\[
		x=\frac{2}{3}y,\ y\in\mathbb{R},\ z=-x.
	\]
	Verifying the solution using the representative vector $\vec{v}=\colvec{2;3;-2}$:
	\[
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}\colvec{2;3;-2} = \colvec{5\cdot2-6\cdot3+3\cdot2;0\cdot2-1\cdot3+0\cdot(-2);0\cdot2+2\cdot3+2\cdot(-2)} = \colvec{10-18+6;-3;6-4} = \colvec{-2;-3;2} = -\colvec{2;3;-2}.
	\]

	\item[$\bm{\lambda_{3}=2}$] solving $A\vec{v}=2\vec{v}$ yields
	\[
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}\colvec{x;y;z} = 2\colvec{x;y;z} = \colvec{2x;2y;2z},
	\]
	i.e.
	\[
		\begin{cases}
			5x-6y-3z &= 2x,\\
			0x-1y+0z &= 2y,\\
			0x+2y+2z &= 2z,
		\end{cases}
	\]
	for which the solution is
	\[
		x=z,\ y=0.
	\]
	Verifying the solution using the representative vector $\colvec{1;0;1}$:
	\[
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}\colvec{1;0;1} = \colvec{5\cdot1-6\cdot0-3\cdot1;0\cdot1-1\cdot0+0\cdot0;0\cdot1+2\cdot0+2\cdot1} = \colvec{2;0;2} = 2\colvec{1;0;1}.
	\]
	\end{descitemize}

	To summarize: the linear transformation represented by the matrix
	\[
		A=
		\begin{bmatrix}
			5 & -6 & -3\\
			0 & -1 &  0\\
			0 &  2 &  2\\
		\end{bmatrix}
	\]
	has three families of eigenvectors:

	\begin{center}
		\begin{tabular}{lc}
			\toprule
			Eigenvalue & Eigenvector \\
			\midrule
			\addlinespace[1em]
			$\lambda_{1}= 5$ & $\colvec{1;0;0}$ \\[3em]
			$\lambda_{2}=-1$ & $\colvec{2;3;-2}$ \\[3em]
			$\lambda_{3}= 2$ & $\colvec{1;0;1}$ \\[2em]
			\bottomrule
		\end{tabular}
	\end{center}
\end{example}

\subsection{Characteristic polynomial}
Did you notice that in both above examples the expression $|A-\lambda I|$ is a polynomial in $\lambda$? This is not a coincidence - any expression of this form is a polynomial in $\lambda$, its degree depending on the form of $A$. We call this polynomial the \emph{characteristic polynomial} of $A$ and its roots are of course the eigenvecors of $A$.

TBC
