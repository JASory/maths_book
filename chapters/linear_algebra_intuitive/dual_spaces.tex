\section{Dual vectors}\label{section:dual_vectors}
\subsection{Functionals}
In the context of linear algebra, a \emph{functional} is a linear function which takes a vector and returns a scalar, e.g. in the case of $\Rs[n]$,
\begin{equation}
	\varphi: \Rs[n] \to \Rs.
	\label{eq:functional_def}
\end{equation}
The linearity of the functional means that given two vectors $\vec{u},\vec{v}\in\Rs[n]$ and two scalars $\alpha,\beta\in\Rs$,
\begin{equation}
	\varphi \left( \alpha\vec{u} + \beta\vec{v} \right) = \alpha\varphi \left( \vec{u} \right) + \beta \left( \vec{v} \right).
	\label{eq:functional_linearity}
\end{equation}

\begin{example}{A functional over $\bm{\Rs[n]}$}{functional in R2}
	Let $\varphi:\Rs[2]\to\Rs$ be defined as
	\[
		\varphi \left( \colvec{x;y} \right) = 2x-y.
	\]
	Then e.g.
	\begin{align*}
		\varphi \left( \colvec{1;0} \right) &= 2\cdot1-0 = 2,\\
		\varphi \left( \colvec{1;-4} \right) &= 2\cdot1-(-4) = 6,\\
		\varphi \left( \colvec{-3;-2} \right) &= 2\cdot(-3)-(-2) = -4,
	\end{align*}
	etc. Let us show that this is indeed a linear function. Given two vectors
	\[
		\vec{u}=\colvec{u_{x};u_{y}}\quad \vec{v}=\colvec{v_{x};v_{y}},
	\]
	and the scalars $\alpha$ and $\beta$,
	\begin{align*}
		\varphi \left( \alpha\vec{u}+\beta\vec{v} \right) &= \varphi \left( \colvec{\alpha u_{x}+\beta v_{x}; \alpha u_{y}+\beta v_{y}} \right)\\
														  &= 2 \left( \alpha u_{x}+\beta v_{x} \right) - \left( \alpha u_{y} + \beta v_{y} \right)\\
														  &= 2\alpha u_{x} + 2\beta v_{x} - \alpha u_{y} - \beta v_{y}\\
														  &= \left(2\alpha u_{x} - \alpha u_{y}\right) + \left( 2\beta v_{x} - \beta v_{y}\right)\\
														  &= \alpha \left( 2u_{x}-u_{y} \right) + \beta \left( 2v_{x}-v_{y} \right)\\
														  &= \alpha \varphi \left( \vec{u} \right) + \beta \varphi \left( \vec{v} \right).
	\end{align*}
\end{example}

Since functionals are linear, the most general functional over $\Rs[n]$, when acting on a general vector
\begin{equation}
	\vec{v}=\colvec{v_{1};v_{2};\vdots;v_{n}}
	\label{eq:general_vec_Rn}
\end{equation}
gives the following output:
\begin{equation}
	\varphi \left( \colvec{v_{1};v_{2};\vdots;v_{n}} \right) = \alpha_{1}v_{1} + \alpha_{2} v_{2} + \cdots + \alpha_{n} v_{n} = \sum\limits_{i=1}^{n}\alpha_{i} v_{i}.
	\label{eq:general_functional}
\end{equation}

\subsection{Duality}
By a closer examination of \autoref{eq:general_functional} we notice that it can be written as a scalar product of the vector
\begin{equation}
	\vec{\alpha}=\colvec{\alpha_{1};\alpha_{2};\vdots;\alpha_{n}}
	\label{eq:alpha_vector_Rn}
\end{equation}
and the vector $\vec{v}$ as defined in \autoref{eq:general_vec_Rn}. This means that in a sense, applying a functional to a vector $\vec{v}$ is identical to performing a scalar product of some coefficient vector and $\vec{v}$. In fact, we can actually define the functional $\varphi$ as defined in \autoref{eq:general_functional} via the scalar product, i.e.
\[
	\varphi \left( \vec{v} \right) = \vec{\alpha} \cdot \vec{v}.
\]

\begin{example}{Functional as vector}{}
	The functional defined in \autoref{example:functional in R2} can be defined via the vector
	\[
		\vec{\alpha} = \colvec{2;-1},
	\]
	since for a general vector
	\[
		\vec{v} = \colvec{x;y}
	\]
	the dot product $\vec{\alpha}\cdot\vec{v}$ yields precisely the same result:
	\[
		\vec{\alpha}\cdot\vec{v} = 2x-y = \varphi \left( \colvec{x;y} \right).
	\]
\end{example}

Since over $\Rs[n]$ a functional can be represented by a scalar product of a vector with specific components, we call the functional a \emph{dual vector}, and the space of all dual vectors the \emph{dual space} of $\Rs[n]$.

Generally speaking, any vector space $V$ has a dual space, which is denoted by $\sconj{V}$. In the case of $\Rs[n]$ this dual space is actually $\Rs[n]$ itself, since any vector in $\Rs[n]$ can be used as a dual vector, and any dual vector is essentially a list of $n$ real components - i.e. is equivalent to a vector in $\Rs[n]$.

\begin{note}{When a functional has less than $\bm{n}$ coefficients}
	Consider the following functional over $\Rs[3]$:
	\[
		\varphi \left( \colvec{x;y;z} \right) = 5x+3z.
	\]
	It seems to have only two coefficients, namely $\alpha_{1}=5$ and $\alpha_{2}=3$ - however, in order to represent it as a vector in $\Rs[3]$ we need three coefficients, i.e. we say that $\alpha_{2}=0$ and $\alpha_{3}=3$ - essentially, we use the vector
	\[
		\vec{\alpha} = \colvec{5;0;3}.
	\]
	to represent it.
\end{note}

\subsection{Row vectors and the outer product}
In order to keep things consistent, we actually don't represent dual-vectors as column vectors but instead as row vectors. This allows us to treat the scalar product between two vectors similarily to the product of two matrices: we always make sure that the vector on the left is written as a row vector, and the vector on the right is written as a column vector, i.e. given
\[
	\vec{u}=\colvec{u_{1};u_{2};\vdots;u_{n}},\quad \vec{v}=\colvec{v_{1};v_{2};\vdots;v_{n}}
\]
we write the scalar product between the two vectors as
\begin{align}
	\vec{u}\cdot\vec{v} &= \rowvec{u_{1};u_{2};\dots;u_{n}}\colvec{v_{1};v_{2};\vdots;v_{n}}\nonumber,\\
	\vec{v}\cdot\vec{u} &= \rowvec{v_{1};v_{2};\dots;v_{n}}\colvec{u_{1};u_{2};\vdots;u_{n}}.
	\label{eq:row_column_dot_product}
\end{align}
(note how the value of the scalar product of $\vec{u}$ and $\vec{v}$ doesn't depend on the order of the vectors, since real numbers are commutative)
The ``matrices'' in question have the dimensions $1 \times n$ and $n \times 1$ respectively, and thus we can calculate their product, which would be a $1 \times 1$ matrix that we interpret as a scalar. This gives rise to the question of what happens if we put a column vector on the left and a row vector on the right? In that case, treating them as ``matrices'' would mean that we are calculating the product of an $n \times 1$ matrix and a $1 \times n$ matrix, and thus the result should be an $n \times n$ matrix.

Indeed, this is what we call the \emph{outer product} of two vectors. Let us examine how the outer product is structured with a simple case:
\[
	\vec{u}=\colvec{2;3},\quad \vec{v}=\rowvec{5;-1}.
\]
Then
\begin{equation}
	\vec{u}\cdot\vec{v} = \colvec{2;3}\rowvec{5;-1} = \begin{bmatrix} 2\cdot5 & 2\cdot(-1) \\ 3\cdot5 & 3\cdot(-1) \end{bmatrix} = \begin{bmatrix} 10&-2 \\ 15&-3 \end{bmatrix}.
	\label{eq:outer_product_simple_example}
\end{equation}

And generaly, for any two vectors in $\Rs[n]$:
\begin{equation}
	\colvec{\Ui{1};\Ui{2};\vdots;\Ui{n}}\rowvec{\Vj{1};\Vj{2};\dots;\Vj{n}} =
	\begin{bNiceMatrix}
		\Ui{1}\Vj{1} & \Ui{1}\Vj{2} & \dots & \Ui{1}\Vj{n}\\
		\Ui{2}\Vj{1} & \Ui{2}\Vj{2} & \dots & \Ui{2}\Vj{n}\\
		\vdots & \vdots & \Ddots & \vdots\\
		\Ui{n}\Vj{1} & \Ui{n}\Vj{2} & \dots & \Ui{n}\Vj{n}\\
	\end{bNiceMatrix},
	\label{eq:outer_product}
\end{equation}
i.e. the value of any element $\Ma{i}{j}$ in the outer product is equal to
\begin{equation}
	\Ma{i}{j} = \Ui{i}\Vj{j}.
	\label{eq:}
\end{equation}

\begin{note}{The inner vs. outer product}{}
	Recall that the scalar product is sometimes called the inner product. If we consider scalars as having some rank equal to $0$, vectors having a rank of $1$ and matrices a rank of $2$ - the inner product reduces the rank of the two components from $1$ to $0$, while the outer product increases them from $1$ to $2$.

	\vspace{1em}
	In that sense, the normal product of two real numbers and the matrix-matrix product are found somewhere inbetween the inner and outer products, as the ranks of their outputs are equal to the ranks of their inputs.
\end{note}

\subsection{A bit about more general vector spaces and their duals}
\tbw{the subsection.}
